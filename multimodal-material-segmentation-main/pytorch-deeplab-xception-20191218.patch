diff -rdNuB -x .git pytorch-deeplab-xception/README.md public-multimodal-material-segmentation/README.md
--- pytorch-deeplab-xception/README.md	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/README.md	2022-03-24 20:46:53.082641726 +0900
@@ -1,83 +1,94 @@
-# pytorch-deeplab-xception
-
-**Update on 2018/12/06. Provide model trained on VOC and SBD datasets.**  
-
-**Update on 2018/11/24. Release newest version code, which fix some previous issues and also add support for new backbones and multi-gpu training. For previous code, please see in `previous` branch**  
-
-### TODO
-- [x] Support different backbones
-- [x] Support VOC, SBD, Cityscapes and COCO datasets
-- [x] Multi-GPU training
-
-
-
-| Backbone  | train/eval os  |mIoU in val |Pretrained Model|
-| :-------- | :------------: |:---------: |:--------------:|
-| ResNet    | 16/16          | 78.43%     | [google drive](https://drive.google.com/open?id=1NwcwlWqA-0HqAPk3dSNNPipGMF0iS0Zu) |
-| MobileNet | 16/16          | 70.81%     | [google drive](https://drive.google.com/open?id=1G9mWafUAj09P4KvGSRVzIsV_U5OqFLdt) |
-| DRN       | 16/16          | 78.87%     | [google drive](https://drive.google.com/open?id=131gZN_dKEXO79NknIQazPJ-4UmRrZAfI) |
-
-
-
-### Introduction
-This is a PyTorch(0.4.1) implementation of [DeepLab-V3-Plus](https://arxiv.org/pdf/1802.02611). It
-can use Modified Aligned Xception and ResNet as backbone. Currently, we train DeepLab V3 Plus
-using Pascal VOC 2012, SBD and Cityscapes datasets.
-
-![Results](doc/results.png)
-
-
-### Installation
-The code was tested with Anaconda and Python 3.6. After installing the Anaconda environment:
+# Multimodal Material Segmentation, CVPR2022
 
-0. Clone the repo:
-    ```Shell
-    git clone https://github.com/jfzhang95/pytorch-deeplab-xception.git
-    cd pytorch-deeplab-xception
-    ```
+This repository provides an inplementation of our paper [Multimodal Material Segmentation]( ) in CVPR 2022.  If you find our work useful in your research please consider citing our paper.
+```
+@InProceedings{Liang_2022_CVPR,
+    author    = {Yupeng Liang and Ryosuke Wakaki and Shohei Nobuhara and Ko Nishino},
+    title     = {Multimodal Material Segmentation},
+    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
+    month     = {June},
+    year      = {2022},
+    pages     = {}
+}
+```
 
-1. Install dependencies:
 
-    For PyTorch dependency, see [pytorch.org](https://pytorch.org/) for more details.
+Please note that this is research software and may contain bugs or other issues – please use it at your own risk. If you experience major problems with it, you may contact us, but please note that we do not have the resources to deal with all issues.
 
-    For custom dependencies:
-    ```Shell
-    pip install matplotlib pillow tensorboardX tqdm
-    ```
-### Training
-Follow steps below to train your model:
 
-0. Configure your dataset path in [mypath.py](https://github.com/jfzhang95/pytorch-deeplab-xception/blob/master/mypath.py).
+## Download data
+The Multimodal Material Segmentation dataset (MCubeS dataset) is available in [Google Drive](https://drive.google.com/file/d/14egTCyC0Pampb7imrXVwaDRffHN7FZxh/view?usp=sharing) (`multimodal_dataset.zip`, 9.27GB). Uncompress the folder and move it to `/dataset/multimodal_dataset/`
 
-1. Input arguments: (see full input arguments via python train.py --help):
-    ```Shell
-    usage: train.py [-h] [--backbone {resnet,xception,drn,mobilenet}]
-                [--out-stride OUT_STRIDE] [--dataset {pascal,coco,cityscapes}]
-                [--use-sbd] [--workers N] [--base-size BASE_SIZE]
-                [--crop-size CROP_SIZE] [--sync-bn SYNC_BN]
-                [--freeze-bn FREEZE_BN] [--loss-type {ce,focal}] [--epochs N]
-                [--start_epoch N] [--batch-size N] [--test-batch-size N]
-                [--use-balanced-weights] [--lr LR]
-                [--lr-scheduler {poly,step,cos}] [--momentum M]
-                [--weight-decay M] [--nesterov] [--no-cuda]
-                [--gpu-ids GPU_IDS] [--seed S] [--resume RESUME]
-                [--checkname CHECKNAME] [--ft] [--eval-interval EVAL_INTERVAL]
-                [--no-val]
+The data should organized in the following format:
+```
+multimodal_dataset
+├── polL_dolp
+│   ├──outscene1208_2_0000000150.npy
+│   ├──outscene1208_2_0000000180.npy
+    ...
+├── polL_color
+│   ├──outscene1208_2_0000000150.png
+│   ├──outscene1208_2_0000000180.png
+    ...
+├── polL_aolp_sin
+├── polL_aolp_cos
+├── list_folder
+│   ├──all.txt
+│   ├──test.txt
+│   ├──train.txt
+│   └──val.txt
+├── SS
+├── SSGT4MS
+├── NIR_warped_mask
+├── NIR_warped
+└── GT
+```
+* polL_dolp: dolp data of each image set (represented as .npy file).
+* polL_color: RGB image of each image set.
+* polL_aolp_sin (cos): the sin (cos) value of aolp data of each image set (both represented as .npy file).
+* list_folder: all.txt records the name of all image sets. Train/val/test.txt records the name of image sets assigned to train/val/test set. 
+* SS: semantic annotation of each image set. The semantic labels here are the same as Cityscapes. 
+* SSGT4MS: condensed semantic segmentation annotation of each image set. As depicted in Supplemental Material Section C, the 23 semantic classes of Cityscapes are consolidated into 10 classes. 
+* NIR_warped_mask: mask of illegal region of each images set. Pixels in the white area will be excluded when computing the loss. 
+* NIR_warped: NIR image of each image set.
+* GT: material annotation of each image set. 
+## Details of MCubeS dataset
+### Overview
+MCubeS captures the visual appearance of various materials found in daily outdoor scenes from a viewpoint on a road, pavement, or sidewalk. At each viewpoint, we capture images with three fundamentally different imaging modalities, RGB, polarization (represented as Aolp and Dolp), and near-infrared (NIR). There are 500 images sets in MCubeS dataset, and each pixel in RGB image is labeled as one of 13 material classes. 
+<p align="center"> <img src="img/Fig1.png"> </p>
 
-    ```
+### Material labels
+We define 20 distinct materials by thoroughly examining the data we captured in the MCubeS dataset. Examples of all materials are shown below. 
+<p align="center"> <img src="img/Fig2.png"> </p>
+Here are some annotation examples. 
+<p align="center"> <img src="img/Fig3.png"> </p>
 
-2. To train deeplabv3+ using Pascal VOC dataset and ResNet as backbone:
-    ```Shell
-    bash train_voc.sh
-    ```
-3. To train deeplabv3+ using COCO dataset and ResNet as backbone:
-    ```Shell
-    bash train_coco.sh
-    ```    
+### Statistics
+The pixel numbers of each material class in train, val and test set are shown below. 
+<p align="center"> <img src="img/Fig4.png"> </p>
 
-### Acknowledgement
-[PyTorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding)
+## Usage
+### Prerequisites
 
-[Synchronized-BatchNorm-PyTorch](https://github.com/vacancy/Synchronized-BatchNorm-PyTorch)
+We tested our code with Python 3.6.12 on Ubuntu 18.04 LTS.  Our code depends on the following modules.
+* torch==1.7.1
+* matplotlib==3.3.2
+* numpy==1.19.2
+* opencv_python==4.4.0.44
 
-[drn](https://github.com/fyu/drn)
+To install this package with pip, use the following command:
+```
+$ python3 -m pip install -r requirements.txt
+```
+in your (virtual) environment.
+### Prepare of the semantic segmentation guidance
+Train a semantic segmentation network model by images in `/dataset/multimodal_dataset/polL_color/` and `/dataset/multimodal_dataset/SSGT4MS/`. Put the semantic segmentation result images in `/dataset/multimodal_dataset/SSmask/`.
+### Training and testing
+* To train MCubeSNet, simply run 
+```
+$ sh main_train_multimodal.sh
+```
+* To test MCubeSNet, simply run 
+```
+$ sh main_test_multimodal.sh
+```
Binary files pytorch-deeplab-xception/__pycache__/mypath.cpython-36.pyc and public-multimodal-material-segmentation/__pycache__/mypath.cpython-36.pyc differ
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/__init__.py public-multimodal-material-segmentation/dataloaders/__init__.py
--- pytorch-deeplab-xception/dataloaders/__init__.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/dataloaders/__init__.py	1970-01-01 09:00:00.000000000 +0900
@@ -1,42 +0,0 @@
-from dataloaders.datasets import cityscapes, coco, combine_dbs, pascal, sbd
-from torch.utils.data import DataLoader
-
-def make_data_loader(args, **kwargs):
-
-    if args.dataset == 'pascal':
-        train_set = pascal.VOCSegmentation(args, split='train')
-        val_set = pascal.VOCSegmentation(args, split='val')
-        if args.use_sbd:
-            sbd_train = sbd.SBDSegmentation(args, split=['train', 'val'])
-            train_set = combine_dbs.CombineDBs([train_set, sbd_train], excluded=[val_set])
-
-        num_class = train_set.NUM_CLASSES
-        train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, **kwargs)
-        val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, **kwargs)
-        test_loader = None
-
-        return train_loader, val_loader, test_loader, num_class
-
-    elif args.dataset == 'cityscapes':
-        train_set = cityscapes.CityscapesSegmentation(args, split='train')
-        val_set = cityscapes.CityscapesSegmentation(args, split='val')
-        test_set = cityscapes.CityscapesSegmentation(args, split='test')
-        num_class = train_set.NUM_CLASSES
-        train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, **kwargs)
-        val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, **kwargs)
-        test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, **kwargs)
-
-        return train_loader, val_loader, test_loader, num_class
-
-    elif args.dataset == 'coco':
-        train_set = coco.COCOSegmentation(args, split='train')
-        val_set = coco.COCOSegmentation(args, split='val')
-        num_class = train_set.NUM_CLASSES
-        train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, **kwargs)
-        val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, **kwargs)
-        test_loader = None
-        return train_loader, val_loader, test_loader, num_class
-
-    else:
-        raise NotImplementedError
-
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/custom_transforms.py public-multimodal-material-segmentation/dataloaders/custom_transforms.py
--- pytorch-deeplab-xception/dataloaders/custom_transforms.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/dataloaders/custom_transforms.py	2022-03-24 20:37:30.242648062 +0900
@@ -3,6 +3,7 @@
 import numpy as np
 
 from PIL import Image, ImageOps, ImageFilter
+import cv2
 
 class Normalize(object):
     """Normalize a tensor image with mean and standard deviation.
@@ -51,8 +52,8 @@
         img = sample['image']
         mask = sample['label']
         if random.random() < 0.5:
-            img = img.transpose(Image.FLIP_LEFT_RIGHT)
-            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)
+            img = img[:,::-1]
+            mask = mask[:,::-1]
 
         return {'image': img,
                 'label': mask}
@@ -78,8 +79,9 @@
         img = sample['image']
         mask = sample['label']
         if random.random() < 0.5:
-            img = img.filter(ImageFilter.GaussianBlur(
-                radius=random.random()))
+            radius = random.random()
+            #img = img.filter(ImageFilter.GaussianBlur(radius=random.random()))
+            img = cv2.GaussianBlur(img, (0,0), radius)
 
         return {'image': img,
                 'label': mask}
@@ -96,28 +98,26 @@
         mask = sample['label']
         # random scale (short edge)
         short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))
-        w, h = img.size
+        w, h = img.shape[:2]
         if h > w:
             ow = short_size
             oh = int(1.0 * h * ow / w)
         else:
             oh = short_size
             ow = int(1.0 * w * oh / h)
-        img = img.resize((ow, oh), Image.BILINEAR)
-        mask = mask.resize((ow, oh), Image.NEAREST)
+        img      = cv2.resize(img  ,(ow,oh), interpolation=cv2.INTER_LINEAR)
+        mask     = cv2.resize(mask ,(ow,oh), interpolation=cv2.INTER_NEAREST)
         # pad crop
         if short_size < self.crop_size:
             padh = self.crop_size - oh if oh < self.crop_size else 0
             padw = self.crop_size - ow if ow < self.crop_size else 0
-            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)
-            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=self.fill)
+            
         # random crop crop_size
-        w, h = img.size
-        x1 = random.randint(0, w - self.crop_size)
-        y1 = random.randint(0, h - self.crop_size)
-        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
-        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
-
+        w, h = img.shape[:2]
+        x1 = random.randint(0, max(0, ow - self.crop_size))
+        y1 = random.randint(0, max(0, oh - self.crop_size))
+        img   =   img[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        mask  =  mask[y1:y1+self.crop_size, x1:x1+self.crop_size]
         return {'image': img,
                 'label': mask}
 
@@ -129,15 +129,15 @@
     def __call__(self, sample):
         img = sample['image']
         mask = sample['label']
-        w, h = img.size
+        w, h = img.shape[:2]
         if w > h:
             oh = self.crop_size
             ow = int(1.0 * w * oh / h)
         else:
             ow = self.crop_size
             oh = int(1.0 * h * ow / w)
-        img = img.resize((ow, oh), Image.BILINEAR)
-        mask = mask.resize((ow, oh), Image.NEAREST)
+        img      = cv2.resize(img  ,(ow,oh), interpolation=cv2.INTER_LINEAR)
+        mask     = cv2.resize(mask ,(ow,oh), interpolation=cv2.INTER_NEAREST)
         # center crop
         w, h = img.size
         x1 = int(round((w - self.crop_size) / 2.))
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/custom_transforms_adv.py public-multimodal-material-segmentation/dataloaders/custom_transforms_adv.py
--- pytorch-deeplab-xception/dataloaders/custom_transforms_adv.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/dataloaders/custom_transforms_adv.py	2022-03-24 20:37:30.242648062 +0900
@@ -0,0 +1,302 @@
+import torch
+import random
+import numpy as np
+import cv2
+
+from PIL import Image, ImageOps, ImageFilter
+
+class Normalize(object):
+    """Normalize a tensor image with mean and standard deviation.
+    Args:
+        mean (tuple): means for each channel.
+        std (tuple): standard deviations for each channel.
+    """
+    def __init__(self, mean=(0., 0., 0.), std=(1., 1., 1.)):
+        self.mean = mean
+        self.std = std
+
+    def __call__(self, sample):
+        img = sample['image']
+        mask = sample['label']
+        img = np.array(img).astype(np.float32)
+        mask = np.array(mask).astype(np.float32)
+        img /= 255.0
+        img -= self.mean
+        img /= self.std
+
+        imgs_remapped = sample['images_remapped']
+        imgs_remapped_new = []
+        for img_remapped in imgs_remapped:
+            img_remapped = np.array(img_remapped).astype(np.float32)
+            mask = np.array(mask).astype(np.float32)
+            img_remapped /= 255.0
+            img_remapped -= self.mean
+            img_remapped /= self.std
+            imgs_remapped_new.append(img_remapped)
+
+        return {'image': img,
+                'images_remapped': imgs_remapped_new,
+                'label': mask,
+                'u_map': sample['u_map'],
+                'v_map': sample['v_map'],
+                'mask' : sample['mask']}
+
+
+class ToTensor(object):
+    """Convert ndarrays in sample to Tensors."""
+
+    def __call__(self, sample):
+        # swap color axis because
+        # numpy image: H x W x C
+        # torch image: C X H X W
+        img = sample['image']
+        mask = sample['label']
+        remapmask = sample['mask']
+        img = np.array(img).astype(np.float32).transpose((2, 0, 1))
+        mask = np.array(mask).astype(np.float32)
+
+        img = torch.from_numpy(img).float()
+        mask = torch.from_numpy(mask).float()
+        remapmask = torch.from_numpy(remapmask)
+
+        imgs_remapped = sample['images_remapped']
+        imgs_remapped_new=[]
+        for img_remapped in imgs_remapped:
+            img_remapped = np.array(img_remapped).astype(np.float32).transpose((2, 0, 1))
+            img_remapped = torch.from_numpy(img_remapped).float()
+            imgs_remapped_new.append(img_remapped)
+
+        u_map = sample['u_map']
+        v_map = sample['v_map']
+        u_map = torch.from_numpy(u_map.astype(np.float32)).float()
+        v_map = torch.from_numpy(v_map.astype(np.float32)).float()
+
+        return {'image': img,
+                'images_remapped': imgs_remapped_new,
+                'label': mask,
+                'u_map': u_map,
+                'v_map': v_map,
+                'mask' : remapmask}
+
+
+class RandomHorizontalFlip(object):
+    def __call__(self, sample):
+        img = sample['image']
+        mask = sample['label']
+        imgs_remapped = sample['images_remapped']
+        u_map = sample['u_map']
+        v_map = sample['v_map']
+        remapmask = sample['mask']
+        imgs_remapped_new = []
+        if random.random() < 0.5:
+            img = img.transpose(Image.FLIP_LEFT_RIGHT)
+            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)
+            u_map = u_map[:,::-1]
+            remapmask = remapmask[:,::-1]
+            for img_remapped in imgs_remapped:
+                img_remapped = img_remapped.transpose(Image.FLIP_LEFT_RIGHT)
+                imgs_remapped_new.append(img_remapped)
+        else:
+            imgs_remapped_new = imgs_remapped
+
+        return {'image': img,
+                'images_remapped': imgs_remapped_new,
+                'label': mask,
+                'u_map': u_map,
+                'v_map': v_map,
+                'mask' : remapmask}
+
+
+# class RandomRotate(object):
+#     def __init__(self, degree):
+#         self.degree = degree
+
+#     def __call__(self, sample):
+#         img = sample['image']
+#         mask = sample['label']
+#         rotate_degree = random.uniform(-1*self.degree, self.degree)
+#         img = img.rotate(rotate_degree, Image.BILINEAR)
+#         mask = mask.rotate(rotate_degree, Image.NEAREST)
+
+#         imgs_remapped = sample['images_remapped']
+#         imgs_remapped_new = []
+#         for img_remapped in imgs_remapped:
+#             img_remapped = img_remapped.rotate(rotate_degree, Image.BILINEAR)
+#             imgs_remapped_new.append(img_remapped)
+
+#         return {'image': img,
+#                 'images_remapped': imgs_remapped_new,
+#                 'label': mask}
+
+
+class RandomGaussianBlur(object):
+    def __call__(self, sample):
+        img = sample['image']
+        mask = sample['label']
+        imgs_remapped = sample['images_remapped']
+        imgs_remapped_new = []
+        if random.random() < 0.5:
+            radius = random.random()
+            img = img.filter(ImageFilter.GaussianBlur(radius=radius))
+            for img_remapped in imgs_remapped:
+                img_remapped = img_remapped.filter(ImageFilter.GaussianBlur(radius=radius))
+                imgs_remapped_new.append(img_remapped)
+        else:
+            imgs_remapped_new = imgs_remapped
+
+        return {'image': img,
+                'images_remapped': imgs_remapped_new,
+                'label': mask,
+                'u_map': sample['u_map'],
+                'v_map': sample['v_map'],
+                'mask' : sample['mask']}
+
+
+class RandomScaleCrop(object):
+    def __init__(self, base_size, crop_size, fill=255):
+        self.base_size = base_size
+        self.crop_size = crop_size
+        self.fill = fill
+
+    def __call__(self, sample):
+        img = sample['image']
+        mask = sample['label']
+        remapmask = sample['mask']
+        remapmask = Image.fromarray(remapmask)
+
+        # random scale (short edge)
+        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))
+        w, h = img.size
+        if h > w:
+            ow = short_size
+            oh = int(1.0 * h * ow / w)
+        else:
+            oh = short_size
+            ow = int(1.0 * w * oh / h)
+        img = img.resize((ow, oh), Image.BILINEAR)
+        mask = mask.resize((ow, oh), Image.NEAREST)
+        # pad crop
+        if short_size < self.crop_size:
+            padh = self.crop_size - oh if oh < self.crop_size else 0
+            padw = self.crop_size - ow if ow < self.crop_size else 0
+            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)
+            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=self.fill)
+            remapmask = ImageOps.expand(remapmask, border=(0, 0, padw, padh), fill=0)
+
+        # random crop crop_size
+        w, h = img.size
+        x1 = random.randint(0, w - self.crop_size)
+        y1 = random.randint(0, h - self.crop_size)
+        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+        remapmask = remapmask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+
+        imgs_remapped = sample['images_remapped']
+        imgs_remapped_new = []
+        for img_remapped in imgs_remapped:
+            img_remapped = img_remapped.resize((ow, oh), Image.BILINEAR)
+            if short_size < self.crop_size:
+                padh = self.crop_size - oh if oh < self.crop_size else 0
+                padw = self.crop_size - ow if ow < self.crop_size else 0
+                img_remapped = ImageOps.expand(img_remapped, border=(0, 0, padw, padh), fill=0)
+            # random crop crop_size
+            img_remapped = img_remapped.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+            imgs_remapped_new.append(img_remapped)
+
+        u_map = sample['u_map']
+        v_map = sample['v_map']
+        u_map = cv2.resize(u_map,(ow,oh))
+        v_map = cv2.resize(v_map,(ow,oh))
+        if short_size < self.crop_size:
+            u_map_ = np.zeros((oh+padh,ow+padw))
+            u_map_[:oh,:ow] = u_map
+            u_map = u_map_
+            v_map_ = np.zeros((oh+padh,ow+padw))
+            v_map_[:oh,:ow] = v_map
+            v_map = v_map_
+        u_map = u_map[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        v_map = v_map[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        
+        remapmask = np.array(remapmask)
+        assert remapmask.dtype == np.bool
+        return {'image': img,
+                'images_remapped': imgs_remapped_new,
+                'label': mask,
+                'u_map': u_map,
+                'v_map': v_map,
+                'mask' : remapmask}
+
+
+class FixScaleCrop(object):
+    def __init__(self, crop_size):
+        self.crop_size = crop_size
+
+    def __call__(self, sample):
+        img = sample['image']
+        mask = sample['label']
+        remapmask = sample['mask']
+        remapmask = Image.fromarray(remapmask)
+
+        w, h = img.size
+        if w > h:
+            oh = self.crop_size
+            ow = int(1.0 * w * oh / h)
+        else:
+            ow = self.crop_size
+            oh = int(1.0 * h * ow / w)
+        img = img.resize((ow, oh), Image.BILINEAR)
+        mask = mask.resize((ow, oh), Image.NEAREST)
+        remapmask = remapmask.resize((ow, oh), Image.NEAREST)
+
+        # center crop
+        w, h = img.size
+        x1 = int(round((w - self.crop_size) / 2.))
+        y1 = int(round((h - self.crop_size) / 2.))
+        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+        remapmask = remapmask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+
+        imgs_remapped = sample['images_remapped']
+        imgs_remapped_new = []
+        for img_remapped in imgs_remapped:
+            img_remapped = img_remapped.resize((ow, oh), Image.BILINEAR)
+            img_remapped = img_remapped.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+            imgs_remapped_new.append(img_remapped)
+
+        u_map = sample['u_map']
+        v_map = sample['v_map']
+        u_map = cv2.resize(u_map,(ow,oh))
+        v_map = cv2.resize(v_map,(ow,oh))
+        u_map = u_map[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        v_map = v_map[y1:y1+self.crop_size, x1:x1+self.crop_size]
+
+        remapmask = np.array(remapmask)
+        return {'image': img,
+                'images_remapped': imgs_remapped_new,
+                'label': mask,
+                'u_map': u_map,
+                'v_map': v_map,
+                'mask' : remapmask}
+
+# class FixedResize(object):
+#     def __init__(self, size):
+#         self.size = (size, size)  # size: (h, w)
+
+#     def __call__(self, sample):
+#         img = sample['image']
+#         mask = sample['label']
+
+#         assert img.size == mask.size
+
+#         img = img.resize(self.size, Image.BILINEAR)
+#         mask = mask.resize(self.size, Image.NEAREST)
+
+#         imgs_remapped = sample['images_remapped']
+#         imgs_remapped_new = []
+#         for img_remapped in imgs_remapped:
+#             img_remapped = img_remapped.resize(self.size, Image.BILINEAR)
+#             imgs_remapped_new.appned(img_remapped)
+
+#         return {'image': img,
+#                 'image_remapped': imgs_remapped_new,
+#                 'label': mask}
\ No newline at end of file
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/custom_transforms_multimodal.py public-multimodal-material-segmentation/dataloaders/custom_transforms_multimodal.py
--- pytorch-deeplab-xception/dataloaders/custom_transforms_multimodal.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/dataloaders/custom_transforms_multimodal.py	2022-03-24 20:37:30.242648062 +0900
@@ -0,0 +1,308 @@
+import torch
+import random
+import numpy as np
+import cv2
+
+from PIL import Image, ImageOps, ImageFilter
+
+class Normalize(object):
+    """Normalize a tensor image with mean and standard deviation.
+    Args:
+        mean (tuple): means for each channel.
+        std (tuple): standard deviations for each channel.
+    """
+    def __init__(self, mean=(0., 0., 0.), std=(1., 1., 1.)):
+        self.mean = mean
+        self.std = std
+
+    def __call__(self, sample):
+        img = sample['image']
+        mask = sample['label']
+        img = np.array(img).astype(np.float32)
+        mask = np.array(mask).astype(np.float32)
+        # img /= 255.0
+        img -= self.mean
+        img /= self.std
+
+        nir = sample['nir']
+        nir = np.array(nir).astype(np.float32)
+        # nir /= 255
+
+        return {'image': img,
+                'label': mask,
+                'aolp' : sample['aolp'], 
+                'dolp' : sample['dolp'], 
+                'nir'  : nir, 
+                'nir_mask': sample['nir_mask'],
+                'u_map': sample['u_map'],
+                'v_map': sample['v_map'],
+                'mask':sample['mask']}
+
+
+class ToTensor(object):
+    """Convert ndarrays in sample to Tensors."""
+
+    def __call__(self, sample):
+        # swap color axis because
+        # numpy image: H x W x C
+        # torch image: C X H X W
+        img = sample['image']
+        mask = sample['label']
+        aolp = sample['aolp']
+        dolp = sample['dolp']
+        nir  = sample['nir']
+        nir_mask  = sample['nir_mask']
+        SS=sample['mask']
+
+        img = np.array(img).astype(np.float32).transpose((2, 0, 1))
+        mask = np.array(mask).astype(np.float32)
+        aolp = np.array(aolp).astype(np.float32).transpose((2, 0, 1))
+        dolp = np.array(dolp).astype(np.float32)
+        SS = np.array(SS).astype(np.float32)
+        nir = np.array(nir).astype(np.float32)
+        nir_mask = np.array(nir_mask).astype(np.float32)
+        
+        img = torch.from_numpy(img).float()
+        mask = torch.from_numpy(mask).float()
+        aolp = torch.from_numpy(aolp).float()
+        dolp = torch.from_numpy(dolp).float()
+        SS = torch.from_numpy(SS).float()
+        nir = torch.from_numpy(nir).float()
+        nir_mask = torch.from_numpy(nir_mask).float()
+
+        u_map = sample['u_map']
+        v_map = sample['v_map']
+        u_map = torch.from_numpy(u_map.astype(np.float32)).float()
+        v_map = torch.from_numpy(v_map.astype(np.float32)).float()
+
+        return {'image': img,
+                'label': mask,
+                'aolp' : aolp,
+                'dolp' : dolp,
+                'nir'  : nir,
+                'nir_mask'  : nir_mask,
+                'u_map': u_map,
+                'v_map': v_map,
+                'mask':SS}
+
+
+class RandomHorizontalFlip(object):
+    def __call__(self, sample):
+        img = sample['image']
+        mask = sample['label']
+        aolp = sample['aolp']
+        dolp = sample['dolp']
+        nir  = sample['nir']
+        nir_mask  = sample['nir_mask']
+        u_map = sample['u_map']
+        v_map = sample['v_map']
+        SS=sample['mask']
+        if random.random() < 0.5:
+            # img = img.transpose(Image.FLIP_LEFT_RIGHT)
+            # mask = mask.transpose(Image.FLIP_LEFT_RIGHT)
+            # nir = nir.transpose(Image.FLIP_LEFT_RIGHT)
+
+            img = img[:,::-1]
+            mask = mask[:,::-1]
+            nir = nir[:,::-1]
+            nir_mask = nir_mask[:,::-1]
+            aolp  = aolp[:,::-1]
+            dolp  = dolp[:,::-1]
+            SS  = SS[:,::-1]
+            u_map = u_map[:,::-1]
+
+        return {'image': img,
+                'label': mask,
+                'aolp' : aolp,
+                'dolp' : dolp,
+                'nir'  : nir,
+                'nir_mask'  : nir_mask,
+                'u_map': u_map,
+                'v_map': v_map,
+                'mask':SS}
+
+class RandomGaussianBlur(object):
+    def __call__(self, sample):
+        img = sample['image']
+        mask = sample['label']
+        nir  = sample['nir']
+        if random.random() < 0.5:
+            radius = random.random()
+            # img = img.filter(ImageFilter.GaussianBlur(radius=radius))
+            # nir = nir.filter(ImageFilter.GaussianBlur(radius=radius))
+            img = cv2.GaussianBlur(img, (0,0), radius)
+            nir = cv2.GaussianBlur(nir, (0,0), radius)
+
+        return {'image': img,
+                'label': mask,
+                'aolp' : sample['aolp'], 
+                'dolp' : sample['dolp'], 
+                'nir'  : nir, 
+                'nir_mask': sample['nir_mask'],
+                'u_map': sample['u_map'],
+                'v_map': sample['v_map'],
+                'mask':sample['mask']}
+
+class RandomScaleCrop(object):
+    def __init__(self, base_size, crop_size, fill=255):
+        self.base_size = base_size
+        self.crop_size = crop_size
+        self.fill = fill
+
+    def __call__(self, sample):
+        img = sample['image']
+        mask = sample['label']
+        aolp = sample['aolp']
+        dolp = sample['dolp']
+        nir = sample['nir']
+        nir_mask = sample['nir_mask']
+        SS=sample['mask']
+        # random scale (short edge)
+        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))
+        # w, h = img.size
+        h, w = img.shape[:2]
+        if h > w:
+            ow = short_size
+            oh = int(1.0 * h * ow / w)
+        else:
+            oh = short_size
+            ow = int(1.0 * w * oh / h)
+
+        # pad crop
+        if short_size < self.crop_size:
+            padh = self.crop_size - oh if oh < self.crop_size else 0
+            padw = self.crop_size - ow if ow < self.crop_size else 0
+            
+        # random crop crop_size
+        # w, h = img.size
+        h, w = img.shape[:2]
+
+        # x1 = random.randint(0, w - self.crop_size)
+        # y1 = random.randint(0, h - self.crop_size)
+        x1 = random.randint(0, max(0, ow - self.crop_size))
+        y1 = random.randint(0, max(0, oh - self.crop_size))
+
+        u_map = sample['u_map']
+        v_map = sample['v_map']
+        u_map    = cv2.resize(u_map,(ow,oh))
+        v_map    = cv2.resize(v_map,(ow,oh))
+        aolp     = cv2.resize(aolp ,(ow,oh))
+        dolp     = cv2.resize(dolp ,(ow,oh))
+        SS     = cv2.resize(SS ,(ow,oh))
+        img      = cv2.resize(img  ,(ow,oh), interpolation=cv2.INTER_LINEAR)
+        mask     = cv2.resize(mask ,(ow,oh), interpolation=cv2.INTER_NEAREST)
+        nir      = cv2.resize(nir  ,(ow,oh), interpolation=cv2.INTER_LINEAR)
+        nir_mask = cv2.resize(nir_mask  ,(ow,oh), interpolation=cv2.INTER_NEAREST)
+        if short_size < self.crop_size:
+            u_map_ = np.zeros((oh+padh,ow+padw))
+            u_map_[:oh,:ow] = u_map
+            u_map = u_map_
+            v_map_ = np.zeros((oh+padh,ow+padw))
+            v_map_[:oh,:ow] = v_map
+            v_map = v_map_
+            aolp_ = np.zeros((oh+padh,ow+padw,2))
+            aolp_[:oh,:ow] = aolp
+            aolp = aolp_
+            dolp_ = np.zeros((oh+padh,ow+padw))
+            dolp_[:oh,:ow] = dolp
+            dolp = dolp_
+
+            img_ = np.zeros((oh+padh,ow+padw,3))
+            img_[:oh,:ow] = img
+            img = img_
+            SS_ = np.zeros((oh+padh,ow+padw))
+            SS_[:oh,:ow] = SS
+            SS = SS_
+            mask_ = np.full((oh+padh,ow+padw),self.fill)
+            mask_[:oh,:ow] = mask
+            mask = mask_
+            nir_ = np.zeros((oh+padh,ow+padw))
+            nir_[:oh,:ow] = nir
+            nir = nir_
+            nir_mask_ = np.zeros((oh+padh,ow+padw))
+            nir_mask_[:oh,:ow] = nir_mask
+            nir_mask = nir_mask_
+
+        u_map = u_map[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        v_map = v_map[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        aolp  =  aolp[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        dolp  =  dolp[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        img   =   img[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        mask  =  mask[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        nir   =   nir[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        SS   =   SS[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        nir_mask = nir_mask[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        return {'image': img,
+                'label': mask,
+                'aolp' : aolp,
+                'dolp' : dolp,
+                'nir'  : nir,
+                'nir_mask'  : nir_mask,
+                'u_map': u_map,
+                'v_map': v_map,
+                'mask':SS}
+
+class FixScaleCrop(object):
+    def __init__(self, crop_size):
+        self.crop_size = crop_size
+
+    def __call__(self, sample):
+        img = sample['image']
+        mask = sample['label']
+        aolp = sample['aolp']
+        dolp = sample['dolp']
+        nir = sample['nir']
+        nir_mask = sample['nir_mask']
+        SS = sample['mask']
+
+        # w, h = img.size
+        h, w = img.shape[:2]
+
+        if w > h:
+            oh = self.crop_size
+            ow = int(1.0 * w * oh / h)
+        else:
+            ow = self.crop_size
+            oh = int(1.0 * h * ow / w)
+        # img = img.resize((ow, oh), Image.BILINEAR)
+        # mask = mask.resize((ow, oh), Image.NEAREST)
+        # nir = nir.resize((ow, oh), Image.BILINEAR)
+
+        # center crop
+        # w, h = img.size
+        # h, w = img.shape[:2]
+        x1 = int(round((ow - self.crop_size) / 2.))
+        y1 = int(round((oh - self.crop_size) / 2.))
+        # img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+        # mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+        # nir = nir.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))
+
+        u_map = sample['u_map']
+        v_map = sample['v_map']
+        u_map = cv2.resize(u_map,(ow,oh))
+        v_map = cv2.resize(v_map,(ow,oh))
+        aolp  = cv2.resize(aolp ,(ow,oh))
+        dolp  = cv2.resize(dolp ,(ow,oh))
+        SS  = cv2.resize(SS ,(ow,oh))
+        img   = cv2.resize(img  ,(ow,oh), interpolation=cv2.INTER_LINEAR)
+        mask  = cv2.resize(mask ,(ow,oh), interpolation=cv2.INTER_NEAREST)
+        nir   = cv2.resize(nir  ,(ow,oh), interpolation=cv2.INTER_LINEAR)
+        nir_mask = cv2.resize(nir_mask,(ow,oh), interpolation=cv2.INTER_NEAREST)
+        u_map = u_map[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        v_map = v_map[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        aolp  =  aolp[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        dolp  =  dolp[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        img   =   img[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        mask  =  mask[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        SS  =  SS[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        nir   =   nir[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        nir_mask = nir_mask[y1:y1+self.crop_size, x1:x1+self.crop_size]
+        return {'image': img,
+                'label': mask,
+                'aolp' : aolp,
+                'dolp' : dolp,
+                'nir'  : nir,
+                'nir_mask'  : nir_mask,
+                'u_map': u_map,
+                'v_map': v_map,
+                'mask':SS}
\ No newline at end of file
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/datasets/handmade_dataset.py public-multimodal-material-segmentation/dataloaders/datasets/handmade_dataset.py
--- pytorch-deeplab-xception/dataloaders/datasets/handmade_dataset.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/dataloaders/datasets/handmade_dataset.py	2022-03-24 20:37:30.242648062 +0900
@@ -0,0 +1,173 @@
+from __future__ import print_function, division
+import os
+import cv2
+from PIL import Image
+import numpy as np
+from torch.utils.data import Dataset
+from mypath import Path
+from torchvision import transforms
+from dataloaders import custom_transforms_multimodal_old as tr
+
+class HandmadeDatasetSegmentation(Dataset):
+    NUM_CLASSES = 21
+
+    def __init__(self,
+                 args,
+                 base_dir=Path.db_root_dir('handmade_dataset'),
+                 split='train',
+                 ):
+        """
+        :param base_dir: path to KITTI dataset directory
+        :param split: train/val
+        :param transform: transform to apply
+        """
+        super().__init__()
+        self._base_dir = base_dir
+        self._image_dir = os.path.join(self._base_dir, 'RGB')
+        self._cat_dir = os.path.join(self._base_dir, 'semantic')
+        self._aolp_dir = os.path.join(self._base_dir, 'AoLP')
+        self._dolp_dir = os.path.join(self._base_dir, 'DoLP')
+        self._nir_dir = os.path.join(self._base_dir, 'NIR_warped')
+
+        if isinstance(split, str):
+            self.split = [split]
+        else:
+            split.sort()
+            self.split = split
+
+        self.args = args
+
+        _splits_dir = os.path.join(self._base_dir, self.args.list_folder)
+
+        self.im_ids = []
+        self.images = []
+        self.aolps  = []
+        self.dolps  = []
+        self.nirs   = []
+        self.categories = []
+
+        for splt in self.split:
+            with open(os.path.join(os.path.join(_splits_dir, splt + '.txt')), "r") as f:
+                lines = f.read().splitlines()
+
+            for ii, line in enumerate(lines):
+                _image = os.path.join(self._image_dir, line + ".png")
+                _cat   = os.path.join(self._cat_dir  , line + ".png")
+                _aolp  = os.path.join(self._aolp_dir , line + ".npy")
+                _dolp  = os.path.join(self._dolp_dir , line + ".npy")
+                _nir   = os.path.join(self._nir_dir  , line + ".png")
+
+                assert os.path.isfile(_image)
+                assert os.path.isfile(_cat)
+                
+                self.im_ids.append(line)
+                self.images.append(_image)
+                self.categories.append(_cat)
+                self.aolps.append(_aolp)
+                self.dolps.append(_dolp)
+                self.nirs.append(_nir)
+                
+        assert (len(self.images) == len(self.categories))
+
+        # Display stats
+        print('Number of images in {}: {:d}'.format(split, len(self.images)))
+
+        self.img_h = 1024
+        self.img_w = 1224
+        max_dim = max(self.img_h, self.img_w)
+        u_vec = (np.arange(self.img_w)-self.img_w/2)/max_dim*2
+        v_vec = (np.arange(self.img_h)-self.img_h/2)/max_dim*2
+        self.u_map, self.v_map = np.meshgrid(u_vec, v_vec)
+        
+    def __len__(self):
+        return len(self.images)
+
+
+    def __getitem__(self, index):
+        _img, _target, _aolp, _dolp, _nir = self._make_img_gt_point_pair(index)
+        sample = {'image': _img, 'label': _target, 'aolp': _aolp, 'dolp': _dolp, 'nir': _nir, 'u_map': self.u_map, 'v_map': self.v_map}
+
+        for split in self.split:
+            if split == "train":
+                return self.transform_tr(sample)
+            elif split == 'val':
+                return self.transform_val(sample)
+
+
+    def _make_img_gt_point_pair(self, index):
+        # _img = Image.open(self.images[index]).convert('RGB')
+        _img = cv2.imread(self.images[index],-1)
+        _img = _img.astype(np.float32)/65535 if _img.dtype==np.uint16 else _img.astype(np.float32)/255
+        # _target = Image.open(self.categories[index])
+        _target = cv2.imread(self.categories[index],-1)
+        _aolp = np.load(self.aolps[index])
+        _aolp = np.stack([np.sin(_aolp), np.cos(_aolp)], axis=2) # H x W x 2
+        _dolp = np.load(self.dolps[index])
+        # _nir  = Image.open(self.nirs[index])
+        _nir  = cv2.imread(self.nirs[index],-1)
+        _nir = _nir.astype(np.float32)/65535 if _nir.dtype==np.uint16 else _nir.astype(np.float32)/255
+        return _img, _target, _aolp, _dolp, _nir
+
+    def transform_tr(self, sample):
+        composed_transforms = transforms.Compose([
+            tr.RandomHorizontalFlip(),
+            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),
+            tr.RandomGaussianBlur(),
+            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
+            tr.ToTensor()])
+
+        return composed_transforms(sample)
+
+    def transform_val(self, sample):
+        composed_transforms = transforms.Compose([
+            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),
+            # tr.FixScaleCrop(crop_size=self.args.crop_size),
+            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
+            tr.ToTensor()])
+
+        return composed_transforms(sample)
+
+    def __str__(self):
+        return 'KITTI_material_dataset(split=' + str(self.split) + ')'
+
+
+if __name__ == '__main__':
+    from dataloaders.utils import decode_segmap
+    from torch.utils.data import DataLoader
+    import matplotlib.pyplot as plt
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    args = parser.parse_args()
+    args.base_size = 513
+    args.crop_size = 513
+
+    voc_train = VOCSegmentation(args, split='train')
+
+    dataloader = DataLoader(voc_train, batch_size=5, shuffle=True, num_workers=0)
+
+    for ii, sample in enumerate(dataloader):
+        for jj in range(sample["image"].size()[0]):
+            img = sample['image'].numpy()
+            gt = sample['label'].numpy()
+            tmp = np.array(gt[jj]).astype(np.uint8)
+            segmap = decode_segmap(tmp, dataset='pascal')
+            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])
+            img_tmp *= (0.229, 0.224, 0.225)
+            img_tmp += (0.485, 0.456, 0.406)
+            img_tmp *= 255.0
+            img_tmp = img_tmp.astype(np.uint8)
+            plt.figure()
+            plt.title('display')
+            plt.subplot(211)
+            plt.imshow(img_tmp)
+            plt.subplot(212)
+            plt.imshow(segmap)
+
+        if ii == 1:
+            break
+
+    plt.show(block=True)
+    # plt.savefig('./out.png')
+
+
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/datasets/handmade_dataset_stereo.py public-multimodal-material-segmentation/dataloaders/datasets/handmade_dataset_stereo.py
--- pytorch-deeplab-xception/dataloaders/datasets/handmade_dataset_stereo.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/dataloaders/datasets/handmade_dataset_stereo.py	2022-03-24 20:37:30.242648062 +0900
@@ -0,0 +1,180 @@
+from __future__ import print_function, division
+import os
+import cv2
+from PIL import Image
+import numpy as np
+from torch.utils.data import Dataset
+from mypath import Path
+from torchvision import transforms
+from dataloaders import custom_transforms_multimodal_old as tr
+
+class HandmadeDatasetSegmentation(Dataset):
+    NUM_CLASSES = 21
+
+    def __init__(self,
+                 args,
+                 base_dir=Path.db_root_dir('handmade_dataset_stereo'),
+                 split='train',
+                 ):
+        """
+        :param base_dir: path to KITTI dataset directory
+        :param split: train/val
+        :param transform: transform to apply
+        """
+        super().__init__()
+        self._base_dir = base_dir
+        self._image_dir = os.path.join(self._base_dir, 'RGB')
+        self._cat_dir = os.path.join(self._base_dir, 'semantic')
+        self._aolp_sin_dir = os.path.join(self._base_dir, 'AoLP_sin')
+        self._aolp_cos_dir = os.path.join(self._base_dir, 'AoLP_cos')
+        self._dolp_dir = os.path.join(self._base_dir, 'DoLP')
+        self._nir_dir = os.path.join(self._base_dir, 'NIR_warped')
+        self._left_offset = 192
+
+        if isinstance(split, str):
+            self.split = [split]
+        else:
+            split.sort()
+            self.split = split
+
+        self.args = args
+
+        _splits_dir = os.path.join(self._base_dir, self.args.list_folder)
+
+        self.im_ids = []
+        self.images = []
+        self.aolp_sins  = []
+        self.aolp_coss  = []
+        self.dolps  = []
+        self.nirs   = []
+        self.categories = []
+
+        for splt in self.split:
+            with open(os.path.join(os.path.join(_splits_dir, splt + '.txt')), "r") as f:
+                lines = f.read().splitlines()
+
+            for ii, line in enumerate(lines):
+                _image = os.path.join(self._image_dir, line + ".png")
+                _cat   = os.path.join(self._cat_dir  , line + ".png")
+                _aolp_sin  = os.path.join(self._aolp_sin_dir , line + ".npy")
+                _aolp_cos  = os.path.join(self._aolp_cos_dir , line + ".npy")
+                _dolp  = os.path.join(self._dolp_dir , line + ".npy")
+                _nir   = os.path.join(self._nir_dir  , line + ".png")
+
+                assert os.path.isfile(_image)
+                assert os.path.isfile(_cat)
+                
+                self.im_ids.append(line)
+                self.images.append(_image)
+                self.categories.append(_cat)
+                self.aolp_sins.append(_aolp_sin)
+                self.aolp_coss.append(_aolp_cos)
+                self.dolps.append(_dolp)
+                self.nirs.append(_nir)
+                
+        assert (len(self.images) == len(self.categories))
+
+        # Display stats
+        print('Number of images in {}: {:d}'.format(split, len(self.images)))
+
+        self.img_h = 1024
+        self.img_w = 1224
+        max_dim = max(self.img_h, self.img_w)
+        u_vec = (np.arange(self.img_w)-self.img_w/2)/max_dim*2
+        v_vec = (np.arange(self.img_h)-self.img_h/2)/max_dim*2
+        self.u_map, self.v_map = np.meshgrid(u_vec, v_vec)
+        self.u_map = self.u_map[:,:self._left_offset]
+        self.v_map = self.v_map[:,:self._left_offset]
+        
+    def __len__(self):
+        return len(self.images)
+
+
+    def __getitem__(self, index):
+        _img, _target, _aolp, _dolp, _nir = self._make_img_gt_point_pair(index)
+        sample = {'image': _img, 'label': _target, 'aolp': _aolp, 'dolp': _dolp, 'nir': _nir, 'u_map': self.u_map, 'v_map': self.v_map}
+
+        for split in self.split:
+            if split == "train":
+                return self.transform_tr(sample)
+            elif split == 'val':
+                return self.transform_val(sample)
+
+
+    def _make_img_gt_point_pair(self, index):
+        _img = cv2.imread(self.images[index],-1)
+        _img = _img.astype(np.float32)/65535 if _img.dtype==np.uint16 else _img.astype(np.float32)/255
+        _target = cv2.imread(self.categories[index],-1)
+        _aolp_sin = np.load(self.aolp_sins[index])
+        _aolp_cos = np.load(self.aolp_coss[index])
+        _aolp = np.stack([_aolp_sin, _aolp_cos], axis=2) # H x W x 2
+        _dolp = np.load(self.dolps[index])
+        _nir  = cv2.imread(self.nirs[index],-1)
+        _nir = _nir.astype(np.float32)/65535 if _nir.dtype==np.uint16 else _nir.astype(np.float32)/255
+        return _img[:,:self._left_offset], _target[:,:self._left_offset], \
+               _aolp[:,:self._left_offset], _dolp[:,:self._left_offset], \
+               _nir[:,:self._left_offset]
+
+    def transform_tr(self, sample):
+        composed_transforms = transforms.Compose([
+            tr.RandomHorizontalFlip(),
+            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),
+            tr.RandomGaussianBlur(),
+            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
+            tr.ToTensor()])
+
+        return composed_transforms(sample)
+
+    def transform_val(self, sample):
+        composed_transforms = transforms.Compose([
+            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),
+            # tr.FixScaleCrop(crop_size=self.args.crop_size),
+            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
+            tr.ToTensor()])
+
+        return composed_transforms(sample)
+
+    def __str__(self):
+        return 'KITTI_material_dataset(split=' + str(self.split) + ')'
+
+
+if __name__ == '__main__':
+    from dataloaders.utils import decode_segmap
+    from torch.utils.data import DataLoader
+    import matplotlib.pyplot as plt
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    args = parser.parse_args()
+    args.base_size = 513
+    args.crop_size = 513
+
+    voc_train = VOCSegmentation(args, split='train')
+
+    dataloader = DataLoader(voc_train, batch_size=5, shuffle=True, num_workers=0)
+
+    for ii, sample in enumerate(dataloader):
+        for jj in range(sample["image"].size()[0]):
+            img = sample['image'].numpy()
+            gt = sample['label'].numpy()
+            tmp = np.array(gt[jj]).astype(np.uint8)
+            segmap = decode_segmap(tmp, dataset='pascal')
+            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])
+            img_tmp *= (0.229, 0.224, 0.225)
+            img_tmp += (0.485, 0.456, 0.406)
+            img_tmp *= 255.0
+            img_tmp = img_tmp.astype(np.uint8)
+            plt.figure()
+            plt.title('display')
+            plt.subplot(211)
+            plt.imshow(img_tmp)
+            plt.subplot(212)
+            plt.imshow(segmap)
+
+        if ii == 1:
+            break
+
+    plt.show(block=True)
+    # plt.savefig('./out.png')
+
+
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/datasets/kitti.py public-multimodal-material-segmentation/dataloaders/datasets/kitti.py
--- pytorch-deeplab-xception/dataloaders/datasets/kitti.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/dataloaders/datasets/kitti.py	2022-03-24 20:37:30.242648062 +0900
@@ -0,0 +1,146 @@
+from __future__ import print_function, division
+import os
+from PIL import Image
+import numpy as np
+from torch.utils.data import Dataset
+from mypath import Path
+from torchvision import transforms
+from dataloaders import custom_transforms as tr
+
+class VOCSegmentation(Dataset):
+    """
+    PascalVoc dataset
+    """
+    NUM_CLASSES = 21
+
+    def __init__(self,
+                 args,
+                 base_dir=Path.db_root_dir('kitti'),
+                 split='train',
+                 ):
+        """
+        :param base_dir: path to VOC dataset directory
+        :param split: train/val
+        :param transform: transform to apply
+        """
+        super().__init__()
+        self._base_dir = base_dir
+        self._image_dir = os.path.join(self._base_dir, 'ori')
+        self._cat_dir = os.path.join(self._base_dir, 'ann_gray')
+
+        if isinstance(split, str):
+            self.split = [split]
+        else:
+            split.sort()
+            self.split = split
+
+        self.args = args
+
+        _splits_dir = os.path.join(self._base_dir, 'list_folder')
+
+        self.im_ids = []
+        self.images = []
+        self.categories = []
+
+        for splt in self.split:
+            with open(os.path.join(os.path.join(_splits_dir, splt + '.txt')), "r") as f:
+                lines = f.read().splitlines()
+
+            for ii, line in enumerate(lines):
+                _image = os.path.join(self._image_dir, line + ".png")
+                _cat = os.path.join(self._cat_dir, line + ".png")
+                assert os.path.isfile(_image)
+                assert os.path.isfile(_cat)
+                self.im_ids.append(line)
+                self.images.append(_image)
+                self.categories.append(_cat)
+
+        assert (len(self.images) == len(self.categories))
+
+        # Display stats
+        print('Number of images in {}: {:d}'.format(split, len(self.images)))
+
+    def __len__(self):
+        return len(self.images)
+
+
+    def __getitem__(self, index):
+        _img, _target = self._make_img_gt_point_pair(index)
+        sample = {'image': _img, 'label': _target}
+
+        for split in self.split:
+            if split == "train":
+                return self.transform_tr(sample)
+            elif split == 'val':
+                return self.transform_val(sample)
+
+
+    def _make_img_gt_point_pair(self, index):
+        _img = Image.open(self.images[index]).convert('RGB')
+        _target = Image.open(self.categories[index])
+
+        return _img, _target
+
+    def transform_tr(self, sample):
+        composed_transforms = transforms.Compose([
+            tr.RandomHorizontalFlip(),
+            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),
+            tr.RandomGaussianBlur(),
+            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
+            tr.ToTensor()])
+
+        return composed_transforms(sample)
+
+    def transform_val(self, sample):
+
+        composed_transforms = transforms.Compose([
+            tr.FixScaleCrop(crop_size=self.args.crop_size),
+            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
+            tr.ToTensor()])
+
+        return composed_transforms(sample)
+
+    def __str__(self):
+        return 'KITTI_material_dataset(split=' + str(self.split) + ')'
+
+
+if __name__ == '__main__':
+    from dataloaders.utils import decode_segmap
+    from torch.utils.data import DataLoader
+    import matplotlib.pyplot as plt
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    args = parser.parse_args()
+    args.base_size = 513
+    args.crop_size = 513
+
+    voc_train = VOCSegmentation(args, split='train')
+
+    dataloader = DataLoader(voc_train, batch_size=5, shuffle=True, num_workers=0)
+
+    for ii, sample in enumerate(dataloader):
+        for jj in range(sample["image"].size()[0]):
+            img = sample['image'].numpy()
+            gt = sample['label'].numpy()
+            tmp = np.array(gt[jj]).astype(np.uint8)
+            segmap = decode_segmap(tmp, dataset='pascal')
+            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])
+            img_tmp *= (0.229, 0.224, 0.225)
+            img_tmp += (0.485, 0.456, 0.406)
+            img_tmp *= 255.0
+            img_tmp = img_tmp.astype(np.uint8)
+            plt.figure()
+            plt.title('display')
+            plt.subplot(211)
+            plt.imshow(img_tmp)
+            plt.subplot(212)
+            plt.imshow(segmap)
+
+        if ii == 1:
+            break
+
+    plt.show(block=True)
+    # plt.savefig('./out.png')
+
+
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/datasets/kitti_advanced.py public-multimodal-material-segmentation/dataloaders/datasets/kitti_advanced.py
--- pytorch-deeplab-xception/dataloaders/datasets/kitti_advanced.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/dataloaders/datasets/kitti_advanced.py	2022-03-24 20:37:30.242648062 +0900
@@ -0,0 +1,185 @@
+from __future__ import print_function, division
+import os
+from PIL import Image
+import numpy as np
+from torch.utils.data import Dataset
+from mypath import Path
+from torchvision import transforms
+# from dataloaders import custom_transforms as tr
+from dataloaders import custom_transforms_adv as tr
+
+class KITTIAdvSegmentation(Dataset):
+    # NUM_CLASSES = 21
+    NUM_CLASSES = 20
+
+    def __init__(self,
+                 args,
+                 base_dir=Path.db_root_dir('kitti_advanced'),
+                 split='train',
+                 ):
+        """
+        :param base_dir: path to KITTI dataset directory
+        :param split: train/val
+        :param transform: transform to apply
+        """
+        super().__init__()
+        self._base_dir = base_dir
+        self._image_dir = os.path.join(self._base_dir, 'train', 'image_2')
+        # print("@@@@@@@@@@@@@@@@@@ USE sequences_remapped @@@@@@@@@@@@@@@@@@@")
+        # self._image_remapped_dir = os.path.join(self._base_dir, 'sequences_remapped')
+        # print("@@@@@@@@@@@@@@@@@@ USE sequences_remapped2 @@@@@@@@@@@@@@@@@@@")
+        # self._image_remapped_dir = os.path.join(self._base_dir, 'sequences_remapped2')
+        print("@@@@@@@@@@@@@@@@@@ USE sequences_remapped4 @@@@@@@@@@@@@@@@@@@")
+        self._image_remapped_dir = os.path.join(self._base_dir, 'sequences_remapped4')
+        self._cat_dir = os.path.join(self._base_dir, 'train', 'semantic')
+
+        if isinstance(split, str):
+            self.split = [split]
+        else:
+            split.sort()
+            self.split = split
+
+        self.args = args
+
+        _splits_dir = os.path.join(self._base_dir, self.args.list_folder)
+
+        self.im_ids = []
+        self.images = []
+        self.images_remapped = []
+        self.categories = []
+
+        for splt in self.split:
+            with open(os.path.join(os.path.join(_splits_dir, splt + '.txt')), "r") as f:
+                lines = f.read().splitlines()
+
+            for ii, line in enumerate(lines):
+                scenename = line.rsplit('_',1)[0]
+                frame_id = int(line.rsplit('_',1)[1])
+                # if frame_id == 0 or frame_id == 245:
+                #     continue
+                _image = os.path.join(self._image_dir, line + ".png")
+                _cat = os.path.join(self._cat_dir, line + ".png")
+                assert os.path.isfile(_image)
+                assert os.path.isfile(_cat)
+                _images_remapped = []
+                for i in range(args.propagation):
+                    i += 1
+                    _image_remapped = os.path.join(self._image_remapped_dir, scenename, f"{frame_id-i:010}+{i}.png")
+                    if not os.path.isfile(_image_remapped):
+                        break
+                    # assert os.path.isfile(_image_remapped)
+                    _images_remapped.append(_image_remapped)
+                    _image_remapped = os.path.join(self._image_remapped_dir, scenename, f"{frame_id+i:010}-{i}.png")
+                    # assert os.path.isfile(_image_remapped)
+                    if not os.path.isfile(_image_remapped):
+                        break
+                    _images_remapped.append(_image_remapped)
+                else:
+                    self.im_ids.append(line)
+                    self.images.append(_image)
+                    self.categories.append(_cat)
+                    self.images_remapped.append(_images_remapped)
+                    assert (len(_images_remapped) == 2 * args.propagation)
+
+        assert (len(self.images) == len(self.categories))
+
+        # Display stats
+        print('Number of images in {}: {:d}'.format(split, len(self.images)))
+
+        self.img_h = 320
+        self.img_w = 1216
+        max_dim = max(self.img_h, self.img_w)
+        u_vec = (np.arange(self.img_w)-self.img_w/2)/max_dim*2
+        v_vec = (np.arange(self.img_h)-self.img_h/2)/max_dim*2
+        self.u_map, self.v_map = np.meshgrid(u_vec, v_vec)
+        
+    def __len__(self):
+        return len(self.images)
+
+
+    def __getitem__(self, index):
+        _img, _imgs_remapped, _target, _mask = self._make_img_gt_point_pair(index)
+        sample = {'image': _img, 'images_remapped': _imgs_remapped, 'label': _target, 'u_map': self.u_map, 'v_map': self.v_map, 'mask':_mask}
+
+        for split in self.split:
+            if split == "train":
+                return self.transform_tr(sample)
+            elif split == 'val':
+                return self.transform_val(sample)
+
+
+    def _make_img_gt_point_pair(self, index):
+        _img = Image.open(self.images[index]).convert('RGB')
+        _target = Image.open(self.categories[index])
+        _imgs_remapped = []
+        w, h = _img.size
+        _mask = np.zeros((h,w),dtype=np.bool)
+        for filename in self.images_remapped[index]:
+            _img_remapped = Image.open(filename).convert('RGB')
+            _mask += (np.sum(np.array(_img_remapped),axis=2)==0)
+            _imgs_remapped.append(_img_remapped)
+        return _img, _imgs_remapped, _target, _mask
+
+    def transform_tr(self, sample):
+        composed_transforms = transforms.Compose([
+            tr.RandomHorizontalFlip(),
+            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),
+            tr.RandomGaussianBlur(),
+            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
+            tr.ToTensor()])
+
+        return composed_transforms(sample)
+
+    def transform_val(self, sample):
+
+        composed_transforms = transforms.Compose([
+            # tr.FixScaleCrop(crop_size=self.args.crop_size),
+            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
+            tr.ToTensor()])
+
+        return composed_transforms(sample)
+
+    def __str__(self):
+        return 'KITTI_material_dataset(split=' + str(self.split) + ')'
+
+
+if __name__ == '__main__':
+    from dataloaders.utils import decode_segmap
+    from torch.utils.data import DataLoader
+    import matplotlib.pyplot as plt
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    args = parser.parse_args()
+    args.base_size = 513
+    args.crop_size = 513
+
+    voc_train = VOCSegmentation(args, split='train')
+
+    dataloader = DataLoader(voc_train, batch_size=5, shuffle=True, num_workers=0)
+
+    for ii, sample in enumerate(dataloader):
+        for jj in range(sample["image"].size()[0]):
+            img = sample['image'].numpy()
+            gt = sample['label'].numpy()
+            tmp = np.array(gt[jj]).astype(np.uint8)
+            segmap = decode_segmap(tmp, dataset='pascal')
+            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])
+            img_tmp *= (0.229, 0.224, 0.225)
+            img_tmp += (0.485, 0.456, 0.406)
+            img_tmp *= 255.0
+            img_tmp = img_tmp.astype(np.uint8)
+            plt.figure()
+            plt.title('display')
+            plt.subplot(211)
+            plt.imshow(img_tmp)
+            plt.subplot(212)
+            plt.imshow(segmap)
+
+        if ii == 1:
+            break
+
+    plt.show(block=True)
+    # plt.savefig('./out.png')
+
+
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/datasets/multimodal_dataset.py public-multimodal-material-segmentation/dataloaders/datasets/multimodal_dataset.py
--- pytorch-deeplab-xception/dataloaders/datasets/multimodal_dataset.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/dataloaders/datasets/multimodal_dataset.py	2022-03-24 20:37:30.242648062 +0900
@@ -0,0 +1,196 @@
+from __future__ import print_function, division
+import os
+import cv2
+from PIL import Image
+import numpy as np
+from torch.utils.data import Dataset
+from mypath import Path
+from torchvision import transforms
+from dataloaders import custom_transforms_multimodal as tr
+
+class MultimodalDatasetSegmentation(Dataset):
+    NUM_CLASSES = 20
+
+    def __init__(self,
+                 args,
+                 base_dir=Path.db_root_dir('multimodal_dataset'),
+                 split='train',
+                 ):
+        """
+        :param base_dir: path to KITTI dataset directory
+        :param split: train/val
+        :param transform: transform to apply
+        """
+        super().__init__()
+        self._base_dir = base_dir
+        self._image_dir = os.path.join(self._base_dir, 'polL_color')
+        self._cat_dir = os.path.join(self._base_dir, 'GT')
+        self._mask_dir = os.path.join(self._base_dir, 'SSmask')
+        self._aolp_sin_dir = os.path.join(self._base_dir, 'polL_aolp_sin')
+        self._aolp_cos_dir = os.path.join(self._base_dir, 'polL_aolp_cos')
+        self._dolp_dir = os.path.join(self._base_dir, 'polL_dolp')
+        self._nir_dir = os.path.join(self._base_dir, 'NIR_warped')
+        self._nir_mask_dir = os.path.join(self._base_dir, 'NIR_warped_mask')
+        self._left_offset = 192
+
+        if isinstance(split, str):
+            self.split = [split]
+        else:
+            split.sort()
+            self.split = split
+
+        self.args = args
+
+        _splits_dir = os.path.join(self._base_dir, self.args.list_folder)
+
+        self.im_ids     = []
+        self.images     = []
+        self.aolp_sins  = []
+        self.aolp_coss  = []
+        self.dolps      = []
+        self.nirs       = []
+        self.nir_masks  = []
+        self.categories = []
+        self.mask = []
+
+
+        for splt in self.split:
+            with open(os.path.join(os.path.join(_splits_dir, splt + '.txt')), "r") as f:
+                lines = f.read().splitlines()
+
+            for ii, line in enumerate(lines):
+                _image    = os.path.join(self._image_dir    , line + ".png")
+                _cat      = os.path.join(self._cat_dir      , line + ".png")
+                _aolp_sin = os.path.join(self._aolp_sin_dir , line + ".npy")
+                _aolp_cos = os.path.join(self._aolp_cos_dir , line + ".npy")
+                _dolp     = os.path.join(self._dolp_dir     , line + ".npy")
+                _nir      = os.path.join(self._nir_dir      , line + ".png")
+                _nir_mask = os.path.join(self._nir_mask_dir , line + ".png")
+                _mask=os.path.join(self._mask_dir      , line + ".png")
+                
+                if not os.path.isfile(_image):
+                    continue
+                assert os.path.isfile(_image)
+                assert os.path.isfile(_cat)
+                
+                self.im_ids.append(line)
+                self.images.append(_image)
+                self.categories.append(_cat)
+                self.mask.append(_mask)
+                self.aolp_sins.append(_aolp_sin)
+                self.aolp_coss.append(_aolp_cos)
+                self.dolps.append(_dolp)
+                self.nirs.append(_nir)
+                self.nir_masks.append(_nir_mask)
+                
+        assert (len(self.images) == len(self.categories))
+
+        # Display stats
+        print('Number of images in {}: {:d}'.format(split, len(self.images)))
+
+        self.img_h = 1024
+        self.img_w = 1224
+        max_dim = max(self.img_h, self.img_w)
+        u_vec = (np.arange(self.img_w)-self.img_w/2)/max_dim*2
+        v_vec = (np.arange(self.img_h)-self.img_h/2)/max_dim*2
+        self.u_map, self.v_map = np.meshgrid(u_vec, v_vec)
+        self.u_map = self.u_map[:,:self._left_offset]
+        self.v_map = self.v_map[:,:self._left_offset]
+        
+    def __len__(self):
+        return len(self.images)
+
+
+    def __getitem__(self, index):
+        _img, _target, _aolp, _dolp, _nir, _nir_mask, mask = self._make_img_gt_point_pair(index)
+        sample = {'image': _img, 'label': _target, 'aolp': _aolp, 'dolp': _dolp, 'nir': _nir, 'nir_mask': _nir_mask, 'u_map': self.u_map, 'v_map': self.v_map,'mask':mask}
+
+        for split in self.split:
+            if split == "train":
+                return self.transform_tr(sample)
+            
+            elif split == 'val':
+                return self.transform_val(sample)
+            elif split == 'test':
+                return self.transform_val(sample)
+
+
+    def _make_img_gt_point_pair(self, index):
+        _img = cv2.imread(self.images[index],-1)[:,:,::-1]
+        _img = _img.astype(np.float32)/65535 if _img.dtype==np.uint16 else _img.astype(np.float32)/255
+        _target = cv2.imread(self.categories[index],-1)
+        #_target = np.load(self.categories[index])
+        _mask = cv2.imread(self.mask[index],-1)
+        _aolp_sin = np.load(self.aolp_sins[index])
+        _aolp_cos = np.load(self.aolp_coss[index])
+        _aolp = np.stack([_aolp_sin, _aolp_cos], axis=2) # H x W x 2
+        _dolp = np.load(self.dolps[index])
+        _nir  = cv2.imread(self.nirs[index],-1)
+        _nir = _nir.astype(np.float32)/65535 if _nir.dtype==np.uint16 else _nir.astype(np.float32)/255
+        _nir_mask = cv2.imread(self.nir_masks[index],0)
+        return _img[:,self._left_offset:], _target[:,self._left_offset:], \
+               _aolp[:,self._left_offset:], _dolp[:,self._left_offset:], \
+               _nir[:,self._left_offset:], _nir_mask[:,self._left_offset:],_mask[:,self._left_offset:]
+
+    def transform_tr(self, sample):
+        composed_transforms = transforms.Compose([
+            tr.RandomHorizontalFlip(),
+            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size,fill=255),
+            tr.RandomGaussianBlur(),
+            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
+            tr.ToTensor()])
+
+        return composed_transforms(sample)
+
+    def transform_val(self, sample):
+        composed_transforms = transforms.Compose([
+            tr.FixScaleCrop(crop_size=1024),
+            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
+            tr.ToTensor()])
+
+        return composed_transforms(sample)
+
+    def __str__(self):
+        return 'KITTI_material_dataset(split=' + str(self.split) + ')'
+
+
+if __name__ == '__main__':
+    from dataloaders.utils import decode_segmap
+    from torch.utils.data import DataLoader
+    import matplotlib.pyplot as plt
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    args = parser.parse_args()
+    args.base_size = 513
+    args.crop_size = 513
+
+    voc_train = VOCSegmentation(args, split='train')
+
+    dataloader = DataLoader(voc_train, batch_size=5, shuffle=True, num_workers=0)
+
+    for ii, sample in enumerate(dataloader):
+        for jj in range(sample["image"].size()[0]):
+            img = sample['image'].numpy()
+            gt = sample['label'].numpy()
+            tmp = np.array(gt[jj]).astype(np.uint8)
+            segmap = decode_segmap(tmp, dataset='pascal')
+            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])
+            img_tmp *= (0.229, 0.224, 0.225)
+            img_tmp += (0.485, 0.456, 0.406)
+            img_tmp *= 255.0
+            img_tmp = img_tmp.astype(np.uint8)
+            plt.figure()
+            plt.title('display')
+            plt.subplot(211)
+            plt.imshow(img_tmp)
+            plt.subplot(212)
+            plt.imshow(segmap)
+
+        if ii == 1:
+            break
+
+    plt.show(block=True)
+    # plt.savefig('./out.png')
+
+
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/datasets/sbd.py public-multimodal-material-segmentation/dataloaders/datasets/sbd.py
--- pytorch-deeplab-xception/dataloaders/datasets/sbd.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/dataloaders/datasets/sbd.py	1970-01-01 09:00:00.000000000 +0900
@@ -1,129 +0,0 @@
-from __future__ import print_function, division
-import os
-
-import numpy as np
-import scipy.io
-import torch.utils.data as data
-from PIL import Image
-from mypath import Path
-
-from torchvision import transforms
-from dataloaders import custom_transforms as tr
-
-class SBDSegmentation(data.Dataset):
-    NUM_CLASSES = 21
-
-    def __init__(self,
-                 args,
-                 base_dir=Path.db_root_dir('sbd'),
-                 split='train',
-                 ):
-        """
-        :param base_dir: path to VOC dataset directory
-        :param split: train/val
-        :param transform: transform to apply
-        """
-        super().__init__()
-        self._base_dir = base_dir
-        self._dataset_dir = os.path.join(self._base_dir, 'dataset')
-        self._image_dir = os.path.join(self._dataset_dir, 'img')
-        self._cat_dir = os.path.join(self._dataset_dir, 'cls')
-
-
-        if isinstance(split, str):
-            self.split = [split]
-        else:
-            split.sort()
-            self.split = split
-
-        self.args = args
-
-        # Get list of all images from the split and check that the files exist
-        self.im_ids = []
-        self.images = []
-        self.categories = []
-        for splt in self.split:
-            with open(os.path.join(self._dataset_dir, splt + '.txt'), "r") as f:
-                lines = f.read().splitlines()
-
-            for line in lines:
-                _image = os.path.join(self._image_dir, line + ".jpg")
-                _categ= os.path.join(self._cat_dir, line + ".mat")
-                assert os.path.isfile(_image)
-                assert os.path.isfile(_categ)
-                self.im_ids.append(line)
-                self.images.append(_image)
-                self.categories.append(_categ)
-
-        assert (len(self.images) == len(self.categories))
-
-        # Display stats
-        print('Number of images: {:d}'.format(len(self.images)))
-
-
-    def __getitem__(self, index):
-        _img, _target = self._make_img_gt_point_pair(index)
-        sample = {'image': _img, 'label': _target}
-
-        return self.transform(sample)
-
-    def __len__(self):
-        return len(self.images)
-
-    def _make_img_gt_point_pair(self, index):
-        _img = Image.open(self.images[index]).convert('RGB')
-        _target = Image.fromarray(scipy.io.loadmat(self.categories[index])["GTcls"][0]['Segmentation'][0])
-
-        return _img, _target
-
-    def transform(self, sample):
-        composed_transforms = transforms.Compose([
-            tr.RandomHorizontalFlip(),
-            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),
-            tr.RandomGaussianBlur(),
-            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
-            tr.ToTensor()])
-
-        return composed_transforms(sample)
-
-
-    def __str__(self):
-        return 'SBDSegmentation(split=' + str(self.split) + ')'
-
-
-if __name__ == '__main__':
-    from dataloaders.utils import decode_segmap
-    from torch.utils.data import DataLoader
-    import matplotlib.pyplot as plt
-    import argparse
-
-    parser = argparse.ArgumentParser()
-    args = parser.parse_args()
-    args.base_size = 513
-    args.crop_size = 513
-
-    sbd_train = SBDSegmentation(args, split='train')
-    dataloader = DataLoader(sbd_train, batch_size=2, shuffle=True, num_workers=2)
-
-    for ii, sample in enumerate(dataloader):
-        for jj in range(sample["image"].size()[0]):
-            img = sample['image'].numpy()
-            gt = sample['label'].numpy()
-            tmp = np.array(gt[jj]).astype(np.uint8)
-            segmap = decode_segmap(tmp, dataset='pascal')
-            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])
-            img_tmp *= (0.229, 0.224, 0.225)
-            img_tmp += (0.485, 0.456, 0.406)
-            img_tmp *= 255.0
-            img_tmp = img_tmp.astype(np.uint8)
-            plt.figure()
-            plt.title('display')
-            plt.subplot(211)
-            plt.imshow(img_tmp)
-            plt.subplot(212)
-            plt.imshow(segmap)
-
-        if ii == 1:
-            break
-
-    plt.show(block=True)
\ No newline at end of file
diff -rdNuB -x .git pytorch-deeplab-xception/dataloaders/utils.py public-multimodal-material-segmentation/dataloaders/utils.py
--- pytorch-deeplab-xception/dataloaders/utils.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/dataloaders/utils.py	2022-03-24 20:37:30.242648062 +0900
@@ -24,6 +24,10 @@
     if dataset == 'pascal' or dataset == 'coco':
         n_classes = 21
         label_colours = get_pascal_labels()
+    elif dataset == 'kitti' or dataset == 'kitti_advanced' or dataset == 'kitti_advanced_manta' \
+            or dataset == 'handmade_dataset' or dataset == 'handmade_dataset_stereo' or dataset == 'multimodal_dataset':
+        n_classes = 20
+        label_colours = get_my_labels()
     elif dataset == 'cityscapes':
         n_classes = 19
         label_colours = get_cityscapes_labels()
@@ -37,6 +41,9 @@
         r[label_mask == ll] = label_colours[ll, 0]
         g[label_mask == ll] = label_colours[ll, 1]
         b[label_mask == ll] = label_colours[ll, 2]
+    r[label_mask == 255] = 0
+    g[label_mask == 255] = 0
+    b[label_mask == 255] = 0
     rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))
     rgb[:, :, 0] = r / 255.0
     rgb[:, :, 1] = g / 255.0
@@ -98,4 +105,28 @@
                        [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],
                        [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],
                        [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],
-                       [0, 64, 128]])
\ No newline at end of file
+                       [0, 64, 128]])
+
+def get_my_labels():
+    " r,g,b"
+    return np.array([
+        [ 44, 160,  44], # asphalt
+        [ 31, 119, 180], # concrete
+        [255, 127,  14], # metal
+        [214,  39,  40], # road marking
+        [140,  86,  75], # fabric, leather
+        [127, 127, 127], # glass
+        [188, 189,  34], # plaster
+        [255, 152, 150], # plastic
+        [ 23, 190, 207], # rubber
+        [174, 199, 232], # sand
+        [196, 156, 148], # gravel
+        [197, 176, 213], # ceramic
+        [247, 182, 210], # cobblestone
+        [199, 199, 199], # brick
+        [219, 219, 141], # grass
+        [158, 218, 229], # wood
+        [ 57,  59, 121], # leaf
+        [107, 110, 207], # water
+        [156, 158, 222], # human body
+        [ 99, 121,  57]]) # sky
diff -rdNuB -x .git pytorch-deeplab-xception/doc/deeplab_resnet.py public-multimodal-material-segmentation/doc/deeplab_resnet.py
--- pytorch-deeplab-xception/doc/deeplab_resnet.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/doc/deeplab_resnet.py	1970-01-01 09:00:00.000000000 +0900
@@ -1,315 +0,0 @@
-import math
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.utils.model_zoo as model_zoo
-from modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d
-
-BatchNorm2d = SynchronizedBatchNorm2d
-
-class Bottleneck(nn.Module):
-    expansion = 4
-
-    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):
-        super(Bottleneck, self).__init__()
-        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
-        self.bn1 = BatchNorm2d(planes)
-        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
-                               dilation=dilation, padding=dilation, bias=False)
-        self.bn2 = BatchNorm2d(planes)
-        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
-        self.bn3 = BatchNorm2d(planes * 4)
-        self.relu = nn.ReLU(inplace=True)
-        self.downsample = downsample
-        self.stride = stride
-        self.dilation = dilation
-
-    def forward(self, x):
-        residual = x
-
-        out = self.conv1(x)
-        out = self.bn1(out)
-        out = self.relu(out)
-
-        out = self.conv2(out)
-        out = self.bn2(out)
-        out = self.relu(out)
-
-        out = self.conv3(out)
-        out = self.bn3(out)
-
-        if self.downsample is not None:
-            residual = self.downsample(x)
-
-        out += residual
-        out = self.relu(out)
-
-        return out
-
-class ResNet(nn.Module):
-
-    def __init__(self, nInputChannels, block, layers, os=16, pretrained=False):
-        self.inplanes = 64
-        super(ResNet, self).__init__()
-        if os == 16:
-            strides = [1, 2, 2, 1]
-            dilations = [1, 1, 1, 2]
-            blocks = [1, 2, 4]
-        elif os == 8:
-            strides = [1, 2, 1, 1]
-            dilations = [1, 1, 2, 2]
-            blocks = [1, 2, 1]
-        else:
-            raise NotImplementedError
-
-        # Modules
-        self.conv1 = nn.Conv2d(nInputChannels, 64, kernel_size=7, stride=2, padding=3,
-                                bias=False)
-        self.bn1 = BatchNorm2d(64)
-        self.relu = nn.ReLU(inplace=True)
-        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
-
-        self.layer1 = self._make_layer(block, 64, layers[0], stride=strides[0], dilation=dilations[0])
-        self.layer2 = self._make_layer(block, 128, layers[1], stride=strides[1], dilation=dilations[1])
-        self.layer3 = self._make_layer(block, 256, layers[2], stride=strides[2], dilation=dilations[2])
-        self.layer4 = self._make_MG_unit(block, 512, blocks=blocks, stride=strides[3], dilation=dilations[3])
-
-        self._init_weight()
-
-        if pretrained:
-            self._load_pretrained_model()
-
-    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):
-        downsample = None
-        if stride != 1 or self.inplanes != planes * block.expansion:
-            downsample = nn.Sequential(
-                nn.Conv2d(self.inplanes, planes * block.expansion,
-                          kernel_size=1, stride=stride, bias=False),
-                BatchNorm2d(planes * block.expansion),
-            )
-
-        layers = []
-        layers.append(block(self.inplanes, planes, stride, dilation, downsample))
-        self.inplanes = planes * block.expansion
-        for i in range(1, blocks):
-            layers.append(block(self.inplanes, planes))
-
-        return nn.Sequential(*layers)
-
-    def _make_MG_unit(self, block, planes, blocks=[1, 2, 4], stride=1, dilation=1):
-        downsample = None
-        if stride != 1 or self.inplanes != planes * block.expansion:
-            downsample = nn.Sequential(
-                nn.Conv2d(self.inplanes, planes * block.expansion,
-                          kernel_size=1, stride=stride, bias=False),
-                BatchNorm2d(planes * block.expansion),
-            )
-
-        layers = []
-        layers.append(block(self.inplanes, planes, stride, dilation=blocks[0]*dilation, downsample=downsample))
-        self.inplanes = planes * block.expansion
-        for i in range(1, len(blocks)):
-            layers.append(block(self.inplanes, planes, stride=1, dilation=blocks[i]*dilation))
-
-        return nn.Sequential(*layers)
-
-    def forward(self, input):
-        x = self.conv1(input)
-        x = self.bn1(x)
-        x = self.relu(x)
-        x = self.maxpool(x)
-
-        x = self.layer1(x)
-        low_level_feat = x
-        x = self.layer2(x)
-        x = self.layer3(x)
-        x = self.layer4(x)
-        return x, low_level_feat
-
-    def _init_weight(self):
-        for m in self.modules():
-            if isinstance(m, nn.Conv2d):
-                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
-                m.weight.data.normal_(0, math.sqrt(2. / n))
-            elif isinstance(m, BatchNorm2d):
-                m.weight.data.fill_(1)
-                m.bias.data.zero_()
-
-    def _load_pretrained_model(self):
-        pretrain_dict = model_zoo.load_url('https://download.pytorch.org/models/resnet101-5d3b4d8f.pth')
-        model_dict = {}
-        state_dict = self.state_dict()
-        for k, v in pretrain_dict.items():
-            if k in state_dict:
-                model_dict[k] = v
-        state_dict.update(model_dict)
-        self.load_state_dict(state_dict)
-
-def ResNet101(nInputChannels=3, os=16, pretrained=False):
-    model = ResNet(nInputChannels, Bottleneck, [3, 4, 23, 3], os, pretrained=pretrained)
-    return model
-
-
-class ASPP_module(nn.Module):
-    def __init__(self, inplanes, planes, dilation):
-        super(ASPP_module, self).__init__()
-        if dilation == 1:
-            kernel_size = 1
-            padding = 0
-        else:
-            kernel_size = 3
-            padding = dilation
-        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,
-                                            stride=1, padding=padding, dilation=dilation, bias=False)
-        self.bn = BatchNorm2d(planes)
-        self.relu = nn.ReLU()
-
-        self._init_weight()
-
-    def forward(self, x):
-        x = self.atrous_convolution(x)
-        x = self.bn(x)
-
-        return self.relu(x)
-
-    def _init_weight(self):
-        for m in self.modules():
-            if isinstance(m, nn.Conv2d):
-                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
-                m.weight.data.normal_(0, math.sqrt(2. / n))
-            elif isinstance(m, BatchNorm2d):
-                m.weight.data.fill_(1)
-                m.bias.data.zero_()
-
-
-class DeepLabv3_plus(nn.Module):
-    def __init__(self, nInputChannels=3, n_classes=21, os=16, pretrained=False, freeze_bn=False, _print=True):
-        if _print:
-            print("Constructing DeepLabv3+ model...")
-            print("Backbone: Resnet-101")
-            print("Number of classes: {}".format(n_classes))
-            print("Output stride: {}".format(os))
-            print("Number of Input Channels: {}".format(nInputChannels))
-        super(DeepLabv3_plus, self).__init__()
-
-        # Atrous Conv
-        self.resnet_features = ResNet101(nInputChannels, os, pretrained=pretrained)
-
-        # ASPP
-        if os == 16:
-            dilations = [1, 6, 12, 18]
-        elif os == 8:
-            dilations = [1, 12, 24, 36]
-        else:
-            raise NotImplementedError
-
-        self.aspp1 = ASPP_module(2048, 256, dilation=dilations[0])
-        self.aspp2 = ASPP_module(2048, 256, dilation=dilations[1])
-        self.aspp3 = ASPP_module(2048, 256, dilation=dilations[2])
-        self.aspp4 = ASPP_module(2048, 256, dilation=dilations[3])
-
-        self.relu = nn.ReLU()
-
-        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),
-                                             nn.Conv2d(2048, 256, 1, stride=1, bias=False),
-                                             BatchNorm2d(256),
-                                             nn.ReLU())
-
-        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)
-        self.bn1 = BatchNorm2d(256)
-
-        # adopt [1x1, 48] for channel reduction.
-        self.conv2 = nn.Conv2d(256, 48, 1, bias=False)
-        self.bn2 = BatchNorm2d(48)
-
-        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),
-                                       BatchNorm2d(256),
-                                       nn.ReLU(),
-                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),
-                                       BatchNorm2d(256),
-                                       nn.ReLU(),
-                                       nn.Conv2d(256, n_classes, kernel_size=1, stride=1))
-        if freeze_bn:
-            self._freeze_bn()
-
-    def forward(self, input):
-        x, low_level_features = self.resnet_features(input)
-        x1 = self.aspp1(x)
-        x2 = self.aspp2(x)
-        x3 = self.aspp3(x)
-        x4 = self.aspp4(x)
-        x5 = self.global_avg_pool(x)
-        x5 = F.upsample(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)
-
-        x = torch.cat((x1, x2, x3, x4, x5), dim=1)
-
-        x = self.conv1(x)
-        x = self.bn1(x)
-        x = self.relu(x)
-        x = F.upsample(x, size=(int(math.ceil(input.size()[-2]/4)),
-                                int(math.ceil(input.size()[-1]/4))), mode='bilinear', align_corners=True)
-
-        low_level_features = self.conv2(low_level_features)
-        low_level_features = self.bn2(low_level_features)
-        low_level_features = self.relu(low_level_features)
-
-
-        x = torch.cat((x, low_level_features), dim=1)
-        x = self.last_conv(x)
-        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)
-
-        return x
-
-    def _freeze_bn(self):
-        for m in self.modules():
-            if isinstance(m, BatchNorm2d):
-                m.eval()
-
-    def _init_weight(self):
-        for m in self.modules():
-            if isinstance(m, nn.Conv2d):
-                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
-                m.weight.data.normal_(0, math.sqrt(2. / n))
-            elif isinstance(m, BatchNorm2d):
-                m.weight.data.fill_(1)
-                m.bias.data.zero_()
-
-def get_1x_lr_params(model):
-    """
-    This generator returns all the parameters of the net except for
-    the last classification layer. Note that for each batchnorm layer,
-    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return
-    any batchnorm parameter
-    """
-    b = [model.resnet_features]
-    for i in range(len(b)):
-        for k in b[i].parameters():
-            if k.requires_grad:
-                yield k
-
-
-def get_10x_lr_params(model):
-    """
-    This generator returns all the parameters for the last layer of the net,
-    which does the classification of pixel into classes
-    """
-    b = [model.aspp1, model.aspp2, model.aspp3, model.aspp4, model.conv1, model.conv2, model.last_conv]
-    for j in range(len(b)):
-        for k in b[j].parameters():
-            if k.requires_grad:
-                yield k
-
-
-if __name__ == "__main__":
-    model = DeepLabv3_plus(nInputChannels=3, n_classes=21, os=16, pretrained=True, _print=True)
-    model.eval()
-    image = torch.randn(1, 3, 512, 512)
-    with torch.no_grad():
-        output = model.forward(image)
-    print(output.size())
-
-
-
-
-
-
diff -rdNuB -x .git pytorch-deeplab-xception/doc/deeplab_xception.py public-multimodal-material-segmentation/doc/deeplab_xception.py
--- pytorch-deeplab-xception/doc/deeplab_xception.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/doc/deeplab_xception.py	1970-01-01 09:00:00.000000000 +0900
@@ -1,424 +0,0 @@
-import math
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.utils.model_zoo as model_zoo
-from modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d
-
-BatchNorm2d = SynchronizedBatchNorm2d
-
-class SeparableConv2d(nn.Module):
-    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=0, dilation=1, bias=False):
-        super(SeparableConv2d, self)._init_()
-
-        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding, dilation,
-                               groups=inplanes, bias=bias)
-        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)
-
-    def forward(self, x):
-        x = self.conv1(x)
-        x = self.pointwise(x)
-        return x
-
-
-def fixed_padding(inputs, kernel_size, dilation):
-    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)
-    pad_total = kernel_size_effective - 1
-    pad_beg = pad_total // 2
-    pad_end = pad_total - pad_beg
-    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))
-    return padded_inputs
-
-
-class SeparableConv2d_same(nn.Module):
-    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, bias=False):
-        super(SeparableConv2d_same, self).__init__()
-
-        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, 0, dilation,
-                               groups=inplanes, bias=bias)
-        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)
-
-    def forward(self, x):
-        x = fixed_padding(x, self.conv1.kernel_size[0], dilation=self.conv1.dilation[0])
-        x = self.conv1(x)
-        x = self.pointwise(x)
-        return x
-
-
-class Block(nn.Module):
-    def __init__(self, inplanes, planes, reps, stride=1, dilation=1, start_with_relu=True, grow_first=True, is_last=False):
-        super(Block, self).__init__()
-
-        if planes != inplanes or stride != 1:
-            self.skip = nn.Conv2d(inplanes, planes, 1, stride=stride, bias=False)
-            self.skipbn = BatchNorm2d(planes)
-        else:
-            self.skip = None
-
-        self.relu = nn.ReLU(inplace=True)
-        rep = []
-
-        filters = inplanes
-        if grow_first:
-            rep.append(self.relu)
-            rep.append(SeparableConv2d_same(inplanes, planes, 3, stride=1, dilation=dilation))
-            rep.append(BatchNorm2d(planes))
-            filters = planes
-
-        for i in range(reps - 1):
-            rep.append(self.relu)
-            rep.append(SeparableConv2d_same(filters, filters, 3, stride=1, dilation=dilation))
-            rep.append(BatchNorm2d(filters))
-
-        if not grow_first:
-            rep.append(self.relu)
-            rep.append(SeparableConv2d_same(inplanes, planes, 3, stride=1, dilation=dilation))
-            rep.append(BatchNorm2d(planes))
-
-        if not start_with_relu:
-            rep = rep[1:]
-
-        if stride != 1:
-            rep.append(SeparableConv2d_same(planes, planes, 3, stride=2))
-
-        if stride == 1 and is_last:
-            rep.append(SeparableConv2d_same(planes, planes, 3, stride=1))
-
-
-        self.rep = nn.Sequential(*rep)
-
-    def forward(self, inp):
-        x = self.rep(inp)
-
-        if self.skip is not None:
-            skip = self.skip(inp)
-            skip = self.skipbn(skip)
-        else:
-            skip = inp
-
-        x += skip
-
-        return x
-
-
-class Xception(nn.Module):
-    """
-    Modified Alighed Xception
-    """
-    def __init__(self, inplanes=3, os=16, pretrained=False):
-        super(Xception, self).__init__()
-
-        if os == 16:
-            entry_block3_stride = 2
-            middle_block_dilation = 1
-            exit_block_dilations = (1, 2)
-        elif os == 8:
-            entry_block3_stride = 1
-            middle_block_dilation = 2
-            exit_block_dilations = (2, 4)
-        else:
-            raise NotImplementedError
-
-
-        # Entry flow
-        self.conv1 = nn.Conv2d(inplanes, 32, 3, stride=2, padding=1, bias=False)
-        self.bn1 = BatchNorm2d(32)
-        self.relu = nn.ReLU(inplace=True)
-
-        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)
-        self.bn2 = BatchNorm2d(64)
-
-        self.block1 = Block(64, 128, reps=2, stride=2, start_with_relu=False)
-        self.block2 = Block(128, 256, reps=2, stride=2, start_with_relu=True, grow_first=True)
-        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride, start_with_relu=True, grow_first=True,
-                            is_last=True)
-
-        # Middle flow
-        self.block4  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block5  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block6  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block7  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block8  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block9  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block10 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block11 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block12 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block13 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block14 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block15 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block16 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block17 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block18 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-        self.block19 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)
-
-        # Exit flow
-        self.block20 = Block(728, 1024, reps=2, stride=1, dilation=exit_block_dilations[0],
-                             start_with_relu=True, grow_first=False, is_last=True)
-
-        self.conv3 = SeparableConv2d_same(1024, 1536, 3, stride=1, dilation=exit_block_dilations[1])
-        self.bn3 = BatchNorm2d(1536)
-
-        self.conv4 = SeparableConv2d_same(1536, 1536, 3, stride=1, dilation=exit_block_dilations[1])
-        self.bn4 = BatchNorm2d(1536)
-
-        self.conv5 = SeparableConv2d_same(1536, 2048, 3, stride=1, dilation=exit_block_dilations[1])
-        self.bn5 = BatchNorm2d(2048)
-
-        # Init weights
-        self._init_weight()
-
-        # Load pretrained model
-        if pretrained:
-            self._load_xception_pretrained()
-
-    def forward(self, x):
-        # Entry flow
-        x = self.conv1(x)
-        x = self.bn1(x)
-        x = self.relu(x)
-
-        x = self.conv2(x)
-        x = self.bn2(x)
-        x = self.relu(x)
-
-        x = self.block1(x)
-        low_level_feat = x
-        x = self.block2(x)
-        x = self.block3(x)
-
-        # Middle flow
-        x = self.block4(x)
-        x = self.block5(x)
-        x = self.block6(x)
-        x = self.block7(x)
-        x = self.block8(x)
-        x = self.block9(x)
-        x = self.block10(x)
-        x = self.block11(x)
-        x = self.block12(x)
-        x = self.block13(x)
-        x = self.block14(x)
-        x = self.block15(x)
-        x = self.block16(x)
-        x = self.block17(x)
-        x = self.block18(x)
-        x = self.block19(x)
-
-        # Exit flow
-        x = self.block20(x)
-        x = self.conv3(x)
-        x = self.bn3(x)
-        x = self.relu(x)
-
-        x = self.conv4(x)
-        x = self.bn4(x)
-        x = self.relu(x)
-
-        x = self.conv5(x)
-        x = self.bn5(x)
-        x = self.relu(x)
-
-        return x, low_level_feat
-
-    def _init_weight(self):
-        for m in self.modules():
-            if isinstance(m, nn.Conv2d):
-                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
-                m.weight.data.normal_(0, math.sqrt(2. / n))
-            elif isinstance(m, BatchNorm2d):
-                m.weight.data.fill_(1)
-                m.bias.data.zero_()
-
-    def _load_xception_pretrained(self):
-        pretrain_dict = model_zoo.load_url('http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth')
-        model_dict = {}
-        state_dict = self.state_dict()
-
-        for k, v in pretrain_dict.items():
-            if k in model_dict:
-                if 'pointwise' in k:
-                    v = v.unsqueeze(-1).unsqueeze(-1)
-                if k.startswith('block11'):
-                    model_dict[k] = v
-                    model_dict[k.replace('block11', 'block12')] = v
-                    model_dict[k.replace('block11', 'block13')] = v
-                    model_dict[k.replace('block11', 'block14')] = v
-                    model_dict[k.replace('block11', 'block15')] = v
-                    model_dict[k.replace('block11', 'block16')] = v
-                    model_dict[k.replace('block11', 'block17')] = v
-                    model_dict[k.replace('block11', 'block18')] = v
-                    model_dict[k.replace('block11', 'block19')] = v
-                elif k.startswith('block12'):
-                    model_dict[k.replace('block12', 'block20')] = v
-                elif k.startswith('bn3'):
-                    model_dict[k] = v
-                    model_dict[k.replace('bn3', 'bn4')] = v
-                elif k.startswith('conv4'):
-                    model_dict[k.replace('conv4', 'conv5')] = v
-                elif k.startswith('bn4'):
-                    model_dict[k.replace('bn4', 'bn5')] = v
-                else:
-                    model_dict[k] = v
-        state_dict.update(model_dict)
-        self.load_state_dict(state_dict)
-
-class ASPP_module(nn.Module):
-    def __init__(self, inplanes, planes, dilation):
-        super(ASPP_module, self).__init__()
-        if dilation == 1:
-            kernel_size = 1
-            padding = 0
-        else:
-            kernel_size = 3
-            padding = dilation
-        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,
-                                            stride=1, padding=padding, dilation=dilation, bias=False)
-        self.bn = BatchNorm2d(planes)
-        self.relu = nn.ReLU()
-
-        self._init_weight()
-
-    def forward(self, x):
-        x = self.atrous_convolution(x)
-        x = self.bn(x)
-
-        return self.relu(x)
-
-    def _init_weight(self):
-        for m in self.modules():
-            if isinstance(m, nn.Conv2d):
-                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
-                m.weight.data.normal_(0, math.sqrt(2. / n))
-            elif isinstance(m, BatchNorm2d):
-                m.weight.data.fill_(1)
-                m.bias.data.zero_()
-
-
-class DeepLabv3_plus(nn.Module):
-    def __init__(self, nInputChannels=3, n_classes=21, os=16, pretrained=False, freeze_bn=False, _print=True):
-        if _print:
-            print("Constructing DeepLabv3+ model...")
-            print("Backbone: Xception")
-            print("Number of classes: {}".format(n_classes))
-            print("Output stride: {}".format(os))
-            print("Number of Input Channels: {}".format(nInputChannels))
-        super(DeepLabv3_plus, self).__init__()
-
-        # Atrous Conv
-        self.xception_features = Xception(nInputChannels, os, pretrained)
-
-        # ASPP
-        if os == 16:
-            dilations = [1, 6, 12, 18]
-        elif os == 8:
-            dilations = [1, 12, 24, 36]
-        else:
-            raise NotImplementedError
-
-        self.aspp1 = ASPP_module(2048, 256, dilation=dilations[0])
-        self.aspp2 = ASPP_module(2048, 256, dilation=dilations[1])
-        self.aspp3 = ASPP_module(2048, 256, dilation=dilations[2])
-        self.aspp4 = ASPP_module(2048, 256, dilation=dilations[3])
-
-        self.relu = nn.ReLU()
-
-        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),
-                                             nn.Conv2d(2048, 256, 1, stride=1, bias=False),
-                                             BatchNorm2d(256),
-                                             nn.ReLU())
-
-        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)
-        self.bn1 = BatchNorm2d(256)
-
-        # adopt [1x1, 48] for channel reduction.
-        self.conv2 = nn.Conv2d(128, 48, 1, bias=False)
-        self.bn2 = BatchNorm2d(48)
-
-        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),
-                                       BatchNorm2d(256),
-                                       nn.ReLU(),
-                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),
-                                       BatchNorm2d(256),
-                                       nn.ReLU(),
-                                       nn.Conv2d(256, n_classes, kernel_size=1, stride=1))
-        if freeze_bn:
-            self._freeze_bn()
-
-    def forward(self, input):
-        x, low_level_features = self.xception_features(input)
-        x1 = self.aspp1(x)
-        x2 = self.aspp2(x)
-        x3 = self.aspp3(x)
-        x4 = self.aspp4(x)
-        x5 = self.global_avg_pool(x)
-        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)
-
-        x = torch.cat((x1, x2, x3, x4, x5), dim=1)
-
-        x = self.conv1(x)
-        x = self.bn1(x)
-        x = self.relu(x)
-        x = F.interpolate(x, size=(int(math.ceil(input.size()[-2]/4)),
-                                int(math.ceil(input.size()[-1]/4))), mode='bilinear', align_corners=True)
-
-        low_level_features = self.conv2(low_level_features)
-        low_level_features = self.bn2(low_level_features)
-        low_level_features = self.relu(low_level_features)
-
-
-        x = torch.cat((x, low_level_features), dim=1)
-        x = self.last_conv(x)
-        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)
-
-        return x
-
-    def _freeze_bn(self):
-        for m in self.modules():
-            if isinstance(m, BatchNorm2d):
-                m.eval()
-
-    def _init_weight(self):
-        for m in self.modules():
-            if isinstance(m, nn.Conv2d):
-                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
-                m.weight.data.normal_(0, math.sqrt(2. / n))
-            elif isinstance(m, BatchNorm2d):
-                m.weight.data.fill_(1)
-                m.bias.data.zero_()
-
-def get_1x_lr_params(model):
-    """
-    This generator returns all the parameters of the net except for
-    the last classification layer. Note that for each batchnorm layer,
-    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return
-    any batchnorm parameter
-    """
-    b = [model.xception_features]
-    for i in range(len(b)):
-        for k in b[i].parameters():
-            if k.requires_grad:
-                yield k
-
-
-def get_10x_lr_params(model):
-    """
-    This generator returns all the parameters for the last layer of the net,
-    which does the classification of pixel into classes
-    """
-    b = [model.aspp1, model.aspp2, model.aspp3, model.aspp4, model.conv1, model.conv2, model.last_conv]
-    for j in range(len(b)):
-        for k in b[j].parameters():
-            if k.requires_grad:
-                yield k
-
-
-if __name__ == "__main__":
-    model = DeepLabv3_plus(nInputChannels=3, n_classes=21, os=16, pretrained=True, _print=True)
-    model.eval()
-    image = torch.randn(1, 3, 512, 512)
-    with torch.no_grad():
-        output = model.forward(image)
-    print(output.size())
-
-
-
Binary files pytorch-deeplab-xception/doc/results.png and public-multimodal-material-segmentation/doc/results.png differ
Binary files pytorch-deeplab-xception/img/Fig1.png and public-multimodal-material-segmentation/img/Fig1.png differ
Binary files pytorch-deeplab-xception/img/Fig2.png and public-multimodal-material-segmentation/img/Fig2.png differ
Binary files pytorch-deeplab-xception/img/Fig3.png and public-multimodal-material-segmentation/img/Fig3.png differ
Binary files pytorch-deeplab-xception/img/Fig4.png and public-multimodal-material-segmentation/img/Fig4.png differ
diff -rdNuB -x .git pytorch-deeplab-xception/main_test_multimodal.sh public-multimodal-material-segmentation/main_test_multimodal.sh
--- pytorch-deeplab-xception/main_test_multimodal.sh	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/main_test_multimodal.sh	2022-03-24 20:37:30.252648062 +0900
@@ -0,0 +1,18 @@
+CUDA_VISIBLE_DEVICES=0 python test.py \
+  --backbone resnet_adv \
+  --lr 0.05 \
+  --workers 12 \
+  --epochs 500 \
+  --batch-size 4 \
+  --gpu-ids 0 \
+  --pth-path ./run/multimodal_dataset/MCubeSNet/experiment_0/checkpoint.pth.tar \
+  --eval-interval 1 \
+  --ratio 3 \
+  --loss-type ce \
+  --dataset multimodal_dataset \
+  --list-folder list_folder \
+  --use-pretrained-resnet \
+  --is-multimodal \
+  --use-dolp \
+  --use-aolp \
+  --use-nir
diff -rdNuB -x .git pytorch-deeplab-xception/main_train_multimodal.sh public-multimodal-material-segmentation/main_train_multimodal.sh
--- pytorch-deeplab-xception/main_train_multimodal.sh	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/main_train_multimodal.sh	2022-03-24 20:37:30.252648062 +0900
@@ -0,0 +1,18 @@
+CUDA_VISIBLE_DEVICES=0 python train.py \
+  --backbone resnet_adv\
+  --lr 0.05 \
+  --workers 1 \
+  --epochs 500 \
+  --batch-size 8 \
+  --ratio 3 \
+  --gpu-ids 0 \
+  --checkname MCubeSNet \
+  --eval-interval 1 \
+  --loss-type ce \
+  --dataset multimodal_dataset \
+  --list-folder list_folder \
+  --use-pretrained-resnet \
+  --is-multimodal \
+  --use-nir \
+  --use-aolp \
+  --use-dolp
diff -rdNuB -x .git pytorch-deeplab-xception/modeling/RGFSConv.py public-multimodal-material-segmentation/modeling/RGFSConv.py
--- pytorch-deeplab-xception/modeling/RGFSConv.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/modeling/RGFSConv.py	2022-03-24 20:37:30.252648062 +0900
@@ -0,0 +1,29 @@
+import torch 
+import torch.nn as nn
+import numpy as np
+import torch.nn.functional as F
+
+
+class RGFSConv(nn.Module):
+    def __init__( self, input_channel, output_channel, ratio, kernel_size=3, padding=0, stride=1, bias=True, dilation=1):
+        super(RGFSConv, self).__init__()
+        
+        self.output_channel = output_channel
+        self.conv1 = nn.Conv2d(input_channel, int(output_channel*ratio), kernel_size=kernel_size, padding=padding, stride=stride, bias=bias, dilation=dilation)
+    def forward(self, x, mask):
+        inter_feature = self.conv1(x)
+        inter_feature = inter_feature.permute(0, 2, 3, 1)
+        mask = F.interpolate(torch.unsqueeze(mask, 1), scale_factor=0.25, mode='nearest')
+        mask = torch.squeeze(mask, 1)
+
+        y = torch.zeros((inter_feature.size()[:3] + (self.output_channel,))).permute(0, 3, 1, 2).cuda()
+        for i in range(10):
+            index = torch.zeros((x.size()[0], x.size()[2], x.size()[3], 1)).cuda()
+            index[mask==i] = 1
+            temp = torch.mul(inter_feature, index)
+            sum_ = temp.sum(dim=1).sum(dim=1)
+            _, indices = torch.sort(sum_, descending=True)
+            e, _ = indices[:, :self.output_channel].sort()
+            for j in range(inter_feature.size()[0]):
+                y[j] += temp.permute(0, 3, 1, 2)[j, e[j], :, :]
+        return y
\ No newline at end of file
diff -rdNuB -x .git pytorch-deeplab-xception/modeling/aspp.py public-multimodal-material-segmentation/modeling/aspp.py
--- pytorch-deeplab-xception/modeling/aspp.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/modeling/aspp.py	2022-03-24 20:37:30.252648062 +0900
@@ -41,7 +41,8 @@
         else:
             inplanes = 2048
         if output_stride == 16:
-            dilations = [1, 6, 12, 18]
+            #dilations = [1, 6, 12, 18]
+            dilations = [1, 2, 4, 6]
         elif output_stride == 8:
             dilations = [1, 12, 24, 36]
         else:
diff -rdNuB -x .git pytorch-deeplab-xception/modeling/backbone/__init__.py public-multimodal-material-segmentation/modeling/backbone/__init__.py
--- pytorch-deeplab-xception/modeling/backbone/__init__.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/modeling/backbone/__init__.py	2022-03-24 20:37:30.252648062 +0900
@@ -1,13 +1,19 @@
-from modeling.backbone import resnet, xception, drn, mobilenet
+from modeling.backbone import resnet, xception, drn, mobilenet, resnet_adv, xception_adv
 
-def build_backbone(backbone, output_stride, BatchNorm):
+def build_backbone(backbone, output_stride, BatchNorm, input_dim=3, pretrained=False):
     if backbone == 'resnet':
-        return resnet.ResNet101(output_stride, BatchNorm)
+        return resnet.ResNet101(output_stride, BatchNorm, pretrained=False)
+    elif backbone == 'resnet_adv':
+        return resnet_adv.ResNet101(output_stride, BatchNorm, pretrained=pretrained, input_dim=input_dim)
+    elif backbone == 'resnet_condconv':
+        return resnet_condconv.ResNet101(output_stride, BatchNorm, pretrained=pretrained, input_dim=input_dim)
     elif backbone == 'xception':
         return xception.AlignedXception(output_stride, BatchNorm)
+    elif backbone == 'xception_adv':
+        return xception_adv.AlignedXception(output_stride, BatchNorm, pretrained=pretrained, input_dim=input_dim)
     elif backbone == 'drn':
         return drn.drn_d_54(BatchNorm)
     elif backbone == 'mobilenet':
         return mobilenet.MobileNetV2(output_stride, BatchNorm)
     else:
-        raise NotImplementedError
+        raise NotImplementedError
\ No newline at end of file
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/__init__.cpython-36.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/__init__.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/__init__.cpython-38.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/__init__.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/drn.cpython-36.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/drn.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/drn.cpython-38.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/drn.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/mobilenet.cpython-36.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/mobilenet.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/mobilenet.cpython-38.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/mobilenet.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/resnet.cpython-36.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/resnet.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/resnet.cpython-38.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/resnet.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/resnet_adv.cpython-36.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/resnet_adv.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/resnet_adv.cpython-38.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/resnet_adv.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/xception.cpython-36.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/xception.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/xception.cpython-38.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/xception.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/xception_adv.cpython-36.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/xception_adv.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/backbone/__pycache__/xception_adv.cpython-38.pyc and public-multimodal-material-segmentation/modeling/backbone/__pycache__/xception_adv.cpython-38.pyc differ
diff -rdNuB -x .git pytorch-deeplab-xception/modeling/backbone/resnet_adv.py public-multimodal-material-segmentation/modeling/backbone/resnet_adv.py
--- pytorch-deeplab-xception/modeling/backbone/resnet_adv.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/modeling/backbone/resnet_adv.py	2022-03-24 20:37:30.252648062 +0900
@@ -0,0 +1,163 @@
+import math
+import torch.nn as nn
+import torch.utils.model_zoo as model_zoo
+from modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d
+
+class Bottleneck(nn.Module):
+    expansion = 4
+
+    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, BatchNorm=None):
+        super(Bottleneck, self).__init__()
+        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
+        self.bn1 = BatchNorm(planes)
+        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
+                               dilation=dilation, padding=dilation, bias=False)
+        self.bn2 = BatchNorm(planes)
+        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
+        self.bn3 = BatchNorm(planes * 4)
+        self.relu = nn.ReLU(inplace=True)
+        self.downsample = downsample
+        self.stride = stride
+        self.dilation = dilation
+
+    def forward(self, x):
+        residual = x
+
+        out = self.conv1(x)
+        out = self.bn1(out)
+        out = self.relu(out)
+
+        out = self.conv2(out)
+        out = self.bn2(out)
+        out = self.relu(out)
+
+        out = self.conv3(out)
+        out = self.bn3(out)
+
+        if self.downsample is not None:
+            residual = self.downsample(x)
+
+        out += residual
+        out = self.relu(out)
+
+        return out
+
+class ResNet(nn.Module):
+
+    def __init__(self, block, layers, output_stride, BatchNorm, pretrained=True,input_dim=3):
+        self.inplanes = 64
+        super(ResNet, self).__init__()
+        blocks = [1, 2, 4]
+        if output_stride == 16:
+            strides = [1, 2, 2, 1]
+            dilations = [1, 1, 1, 2]
+        elif output_stride == 8:
+            strides = [1, 2, 1, 1]
+            dilations = [1, 1, 2, 4]
+        else:
+            raise NotImplementedError
+        # Modules
+        self.conv1 = nn.Conv2d(input_dim, 64, kernel_size=7, stride=2, padding=3,
+                                bias=False)
+        self.bn1 = BatchNorm(64)
+        self.relu = nn.ReLU(inplace=True)
+        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+
+        self.layer1 = self._make_layer(block, 64, layers[0], stride=strides[0], dilation=dilations[0], BatchNorm=BatchNorm)
+        self.layer2 = self._make_layer(block, 128, layers[1], stride=strides[1], dilation=dilations[1], BatchNorm=BatchNorm)
+        self.layer3 = self._make_layer(block, 256, layers[2], stride=strides[2], dilation=dilations[2], BatchNorm=BatchNorm)
+        self.layer4 = self._make_MG_unit(block, 512, blocks=blocks, stride=strides[3], dilation=dilations[3], BatchNorm=BatchNorm)
+        # self.layer4 = self._make_layer(block, 512, layers[3], stride=strides[3], dilation=dilations[3], BatchNorm=BatchNorm)
+        self._init_weight()
+
+        if pretrained and input_dim == 3:
+            self._load_pretrained_model()
+    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, BatchNorm=None):
+        downsample = None
+        if stride != 1 or self.inplanes != planes * block.expansion:
+            downsample = nn.Sequential(
+                nn.Conv2d(self.inplanes, planes * block.expansion,
+                          kernel_size=1, stride=stride, bias=False),
+                BatchNorm(planes * block.expansion),
+            )
+
+        layers = []
+        layers.append(block(self.inplanes, planes, stride, dilation, downsample, BatchNorm))
+        self.inplanes = planes * block.expansion
+        for i in range(1, blocks):
+            layers.append(block(self.inplanes, planes, dilation=dilation, BatchNorm=BatchNorm))
+
+        return nn.Sequential(*layers)
+
+    def _make_MG_unit(self, block, planes, blocks, stride=1, dilation=1, BatchNorm=None):
+        downsample = None
+        if stride != 1 or self.inplanes != planes * block.expansion:
+            downsample = nn.Sequential(
+                nn.Conv2d(self.inplanes, planes * block.expansion,
+                          kernel_size=1, stride=stride, bias=False),
+                BatchNorm(planes * block.expansion),
+            )
+
+        layers = []
+        layers.append(block(self.inplanes, planes, stride, dilation=blocks[0]*dilation,
+                            downsample=downsample, BatchNorm=BatchNorm))
+        self.inplanes = planes * block.expansion
+        for i in range(1, len(blocks)):
+            layers.append(block(self.inplanes, planes, stride=1,
+                                dilation=blocks[i]*dilation, BatchNorm=BatchNorm))
+
+        return nn.Sequential(*layers)
+
+    def forward(self, input):
+        
+        x = self.conv1(input)
+        x = self.bn1(x)
+        x = self.relu(x)
+        x = self.maxpool(x)
+
+        x = self.layer1(x)
+        low_level_feat = x
+        x = self.layer2(x)
+        x = self.layer3(x)
+        x = self.layer4(x)
+        return x, low_level_feat
+
+    def _init_weight(self):
+        for m in self.modules():
+            if isinstance(m, nn.Conv2d):
+                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+                m.weight.data.normal_(0, math.sqrt(2. / n))
+            elif isinstance(m, SynchronizedBatchNorm2d):
+                m.weight.data.fill_(1)
+                m.bias.data.zero_()
+            elif isinstance(m, nn.BatchNorm2d):
+                m.weight.data.fill_(1)
+                m.bias.data.zero_()
+
+    def _load_pretrained_model(self):
+        pretrain_dict = model_zoo.load_url('https://download.pytorch.org/models/resnet101-5d3b4d8f.pth')
+        model_dict = {}
+        state_dict = self.state_dict()
+        #print(state_dict.keys())
+        for k, v in pretrain_dict.items():
+            if k in state_dict:
+                #print(k)
+                model_dict[k] = v
+        state_dict.update(model_dict)
+        self.load_state_dict(state_dict)
+
+def ResNet101(output_stride, BatchNorm, pretrained=True, input_dim=3):
+    """Constructs a ResNet-101 model.
+    Args:
+        pretrained (bool): If True, returns a model pre-trained on ImageNet
+    """
+    model = ResNet(Bottleneck, [3, 4, 23, 3], output_stride, BatchNorm, pretrained=pretrained, input_dim=input_dim)
+    return model
+
+if __name__ == "__main__":
+    import torch
+    model = ResNet101(BatchNorm=nn.BatchNorm2d, pretrained=False, output_stride=8)
+    input = torch.rand(1, 9, 512, 512)
+    output, low_level_feat = model(input)
+    print(output.size())
+    print(low_level_feat.size())
\ No newline at end of file
diff -rdNuB -x .git pytorch-deeplab-xception/modeling/backbone/xception_adv.py public-multimodal-material-segmentation/modeling/backbone/xception_adv.py
--- pytorch-deeplab-xception/modeling/backbone/xception_adv.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/modeling/backbone/xception_adv.py	2022-03-24 20:37:30.252648062 +0900
@@ -0,0 +1,289 @@
+import math
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.utils.model_zoo as model_zoo
+from modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d
+
+def fixed_padding(inputs, kernel_size, dilation):
+    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)
+    pad_total = kernel_size_effective - 1
+    pad_beg = pad_total // 2
+    pad_end = pad_total - pad_beg
+    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))
+    return padded_inputs
+
+
+class SeparableConv2d(nn.Module):
+    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, bias=False, BatchNorm=None):
+        super(SeparableConv2d, self).__init__()
+
+        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, 0, dilation,
+                               groups=inplanes, bias=bias)
+        self.bn = BatchNorm(inplanes)
+        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)
+
+    def forward(self, x):
+        x = fixed_padding(x, self.conv1.kernel_size[0], dilation=self.conv1.dilation[0])
+        x = self.conv1(x)
+        x = self.bn(x)
+        x = self.pointwise(x)
+        return x
+
+
+class Block(nn.Module):
+    def __init__(self, inplanes, planes, reps, stride=1, dilation=1, BatchNorm=None,
+                 start_with_relu=True, grow_first=True, is_last=False):
+        super(Block, self).__init__()
+
+        if planes != inplanes or stride != 1:
+            self.skip = nn.Conv2d(inplanes, planes, 1, stride=stride, bias=False)
+            self.skipbn = BatchNorm(planes)
+        else:
+            self.skip = None
+
+        self.relu = nn.ReLU(inplace=True)
+        rep = []
+
+        filters = inplanes
+        if grow_first:
+            rep.append(self.relu)
+            rep.append(SeparableConv2d(inplanes, planes, 3, 1, dilation, BatchNorm=BatchNorm))
+            rep.append(BatchNorm(planes))
+            filters = planes
+
+        for i in range(reps - 1):
+            rep.append(self.relu)
+            rep.append(SeparableConv2d(filters, filters, 3, 1, dilation, BatchNorm=BatchNorm))
+            rep.append(BatchNorm(filters))
+
+        if not grow_first:
+            rep.append(self.relu)
+            rep.append(SeparableConv2d(inplanes, planes, 3, 1, dilation, BatchNorm=BatchNorm))
+            rep.append(BatchNorm(planes))
+
+        if stride != 1:
+            rep.append(self.relu)
+            rep.append(SeparableConv2d(planes, planes, 3, 2, BatchNorm=BatchNorm))
+            rep.append(BatchNorm(planes))
+
+        if stride == 1 and is_last:
+            rep.append(self.relu)
+            rep.append(SeparableConv2d(planes, planes, 3, 1, BatchNorm=BatchNorm))
+            rep.append(BatchNorm(planes))
+
+        if not start_with_relu:
+            rep = rep[1:]
+
+        self.rep = nn.Sequential(*rep)
+
+    def forward(self, inp):
+        x = self.rep(inp)
+
+        if self.skip is not None:
+            skip = self.skip(inp)
+            skip = self.skipbn(skip)
+        else:
+            skip = inp
+
+        x = x + skip
+
+        return x
+
+
+class AlignedXception(nn.Module):
+    """
+    Modified Alighed Xception
+    """
+    def __init__(self, output_stride, BatchNorm,
+                 pretrained=True,input_dim=3):
+        super(AlignedXception, self).__init__()
+
+        if output_stride == 16:
+            entry_block3_stride = 2
+            middle_block_dilation = 1
+            exit_block_dilations = (1, 2)
+        elif output_stride == 8:
+            entry_block3_stride = 1
+            middle_block_dilation = 2
+            exit_block_dilations = (2, 4)
+        else:
+            raise NotImplementedError
+
+
+        # Entry flow
+        self.conv1 = nn.Conv2d(input_dim, 32, 3, stride=2, padding=1, bias=False)
+        self.bn1 = BatchNorm(32)
+        self.relu = nn.ReLU(inplace=True)
+
+        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)
+        self.bn2 = BatchNorm(64)
+
+        self.block1 = Block(64, 128, reps=2, stride=2, BatchNorm=BatchNorm, start_with_relu=False)
+        self.block2 = Block(128, 256, reps=2, stride=2, BatchNorm=BatchNorm, start_with_relu=False,
+                            grow_first=True)
+        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride, BatchNorm=BatchNorm,
+                            start_with_relu=True, grow_first=True, is_last=True)
+
+        # Middle flow
+        self.block4  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block5  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block6  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block7  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block8  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block9  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block10 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block11 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block12 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block13 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block14 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block15 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block16 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block17 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block18 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+        self.block19 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)
+
+        # Exit flow
+        self.block20 = Block(728, 1024, reps=2, stride=1, dilation=exit_block_dilations[0],
+                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=False, is_last=True)
+
+        self.conv3 = SeparableConv2d(1024, 1536, 3, stride=1, dilation=exit_block_dilations[1], BatchNorm=BatchNorm)
+        self.bn3 = BatchNorm(1536)
+
+        self.conv4 = SeparableConv2d(1536, 1536, 3, stride=1, dilation=exit_block_dilations[1], BatchNorm=BatchNorm)
+        self.bn4 = BatchNorm(1536)
+
+        self.conv5 = SeparableConv2d(1536, 2048, 3, stride=1, dilation=exit_block_dilations[1], BatchNorm=BatchNorm)
+        self.bn5 = BatchNorm(2048)
+
+        # Init weights
+        self._init_weight()
+
+        # Load pretrained model
+        if pretrained:
+            self._load_pretrained_model()
+
+    def forward(self, x):
+        # Entry flow
+        x = self.conv1(x)
+        x = self.bn1(x)
+        x = self.relu(x)
+
+        x = self.conv2(x)
+        x = self.bn2(x)
+        x = self.relu(x)
+
+        x = self.block1(x)
+        # add relu here
+        x = self.relu(x)
+        low_level_feat = x
+        x = self.block2(x)
+        x = self.block3(x)
+
+        # Middle flow
+        x = self.block4(x)
+        x = self.block5(x)
+        x = self.block6(x)
+        x = self.block7(x)
+        x = self.block8(x)
+        x = self.block9(x)
+        x = self.block10(x)
+        x = self.block11(x)
+        x = self.block12(x)
+        x = self.block13(x)
+        x = self.block14(x)
+        x = self.block15(x)
+        x = self.block16(x)
+        x = self.block17(x)
+        x = self.block18(x)
+        x = self.block19(x)
+
+        # Exit flow
+        x = self.block20(x)
+        x = self.relu(x)
+        x = self.conv3(x)
+        x = self.bn3(x)
+        x = self.relu(x)
+
+        x = self.conv4(x)
+        x = self.bn4(x)
+        x = self.relu(x)
+
+        x = self.conv5(x)
+        x = self.bn5(x)
+        x = self.relu(x)
+
+        return x, low_level_feat
+
+    def _init_weight(self):
+        for m in self.modules():
+            if isinstance(m, nn.Conv2d):
+                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+                m.weight.data.normal_(0, math.sqrt(2. / n))
+            elif isinstance(m, SynchronizedBatchNorm2d):
+                m.weight.data.fill_(1)
+                m.bias.data.zero_()
+            elif isinstance(m, nn.BatchNorm2d):
+                m.weight.data.fill_(1)
+                m.bias.data.zero_()
+
+
+    def _load_pretrained_model(self):
+        print("Xception : Load pretrained model")
+        pretrain_dict = model_zoo.load_url('http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth')
+        model_dict = {}
+        state_dict = self.state_dict()
+
+        for k, v in pretrain_dict.items():
+            if k in state_dict:
+                if 'pointwise' in k:
+                    v = v.unsqueeze(-1).unsqueeze(-1)
+                if k.startswith('block11'):
+                    model_dict[k] = v
+                    model_dict[k.replace('block11', 'block12')] = v
+                    model_dict[k.replace('block11', 'block13')] = v
+                    model_dict[k.replace('block11', 'block14')] = v
+                    model_dict[k.replace('block11', 'block15')] = v
+                    model_dict[k.replace('block11', 'block16')] = v
+                    model_dict[k.replace('block11', 'block17')] = v
+                    model_dict[k.replace('block11', 'block18')] = v
+                    model_dict[k.replace('block11', 'block19')] = v
+                elif k.startswith('block12'):
+                    model_dict[k.replace('block12', 'block20')] = v
+                elif k.startswith('bn3'):
+                    model_dict[k] = v
+                    model_dict[k.replace('bn3', 'bn4')] = v
+                elif k.startswith('conv4'):
+                    model_dict[k.replace('conv4', 'conv5')] = v
+                elif k.startswith('bn4'):
+                    model_dict[k.replace('bn4', 'bn5')] = v
+                else:
+                    model_dict[k] = v
+        state_dict.update(model_dict)
+        self.load_state_dict(state_dict)
+
+
+
+if __name__ == "__main__":
+    import torch
+    model = AlignedXception(BatchNorm=nn.BatchNorm2d, pretrained=True, output_stride=16)
+    input = torch.rand(1, 3, 512, 512)
+    output, low_level_feat = model(input)
+    print(output.size())
+    print(low_level_feat.size())
diff -rdNuB -x .git pytorch-deeplab-xception/modeling/decoder.py public-multimodal-material-segmentation/modeling/decoder.py
--- pytorch-deeplab-xception/modeling/decoder.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/modeling/decoder.py	2022-03-24 20:37:30.252648062 +0900
@@ -3,35 +3,52 @@
 import torch.nn as nn
 import torch.nn.functional as F
 from modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d
+from modeling.RGFSConv import RGFSConv
+
 
 class Decoder(nn.Module):
-    def __init__(self, num_classes, backbone, BatchNorm):
+    def __init__(self, num_classes, backbone, BatchNorm, ratio, input_heads=1):
         super(Decoder, self).__init__()
-        if backbone == 'resnet' or backbone == 'drn':
+        
+        if backbone == 'resnet' or backbone == 'drn' :
             low_level_inplanes = 256
+            last_conv_input = 304
+        elif backbone == 'resnet_adv'or backbone=='resnet_condconv':
+            low_level_inplanes = 256*input_heads
+            last_conv_input = 256*input_heads + 48
         elif backbone == 'xception':
             low_level_inplanes = 128
+            last_conv_input = 304
+        elif backbone == 'xception_adv':
+            low_level_inplanes = 128*input_heads
+            last_conv_input = 256*input_heads + 48
         elif backbone == 'mobilenet':
             low_level_inplanes = 24
+            last_conv_input = 304
+        elif backbone == 'plus':
+            low_level_inplanes = 256
+            last_conv_input = 304
         else:
             raise NotImplementedError
-
+        
         self.conv1 = nn.Conv2d(low_level_inplanes, 48, 1, bias=False)
         self.bn1 = BatchNorm(48)
         self.relu = nn.ReLU()
-        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),
-                                       BatchNorm(256),
-                                       nn.ReLU(),
-                                       nn.Dropout(0.5),
-                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),
-                                       BatchNorm(256),
-                                       nn.ReLU(),
-                                       nn.Dropout(0.1),
-                                       nn.Conv2d(256, num_classes, kernel_size=1, stride=1))
+        self.condconv1 = RGFSConv(last_conv_input, 256, ratio, kernel_size=3, stride=1, padding=1, bias=False)
+        self.last_conv = nn.Sequential(
+                                    BatchNorm(256),
+                                    nn.ReLU(),
+                                    nn.Dropout(0.5),
+                                    nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),
+                                    BatchNorm(256),
+                                    nn.ReLU(),
+                                    nn.Dropout(0.1),
+                                    nn.Conv2d(256, num_classes, kernel_size=1, stride=1))
         self._init_weight()
 
 
-    def forward(self, x, low_level_feat):
+    def forward(self, x, low_level_feat, mask):
+
         low_level_feat = self.conv1(low_level_feat)
         low_level_feat = self.bn1(low_level_feat)
         low_level_feat = self.relu(low_level_feat)
@@ -35,11 +52,12 @@
         low_level_feat = self.conv1(low_level_feat)
         low_level_feat = self.bn1(low_level_feat)
         low_level_feat = self.relu(low_level_feat)
-
         x = F.interpolate(x, size=low_level_feat.size()[2:], mode='bilinear', align_corners=True)
         x = torch.cat((x, low_level_feat), dim=1)
+        x = self.condconv1(x, mask)
         x = self.last_conv(x)
 
+
         return x
 
     def _init_weight(self):
@@ -53,5 +71,5 @@
                 m.weight.data.fill_(1)
                 m.bias.data.zero_()
 
-def build_decoder(num_classes, backbone, BatchNorm):
-    return Decoder(num_classes, backbone, BatchNorm)
\ No newline at end of file
+def build_decoder(num_classes, backbone, BatchNorm, ratio, input_heads=1):
+    return Decoder(num_classes, backbone, BatchNorm, ratio, input_heads)
\ No newline at end of file
diff -rdNuB -x .git pytorch-deeplab-xception/modeling/deeplab.py public-multimodal-material-segmentation/modeling/deeplab.py
--- pytorch-deeplab-xception/modeling/deeplab.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/modeling/deeplab.py	2022-03-24 20:37:30.252648062 +0900
@@ -6,10 +6,11 @@
 from modeling.decoder import build_decoder
 from modeling.backbone import build_backbone
 
-class DeepLab(nn.Module):
-    def __init__(self, backbone='resnet', output_stride=16, num_classes=21,
-                 sync_bn=True, freeze_bn=False):
-        super(DeepLab, self).__init__()
+
+class DeepLabMultiInput(nn.Module):
+    def __init__(self, backbone='resnet', output_stride=16, num_classes=1,
+                 sync_bn=True, freeze_bn=False, input_dim=3, ratio=1, pretrained=False):
+        super(DeepLabMultiInput, self).__init__()
         if backbone == 'drn':
             output_stride = 8
 
@@ -17,18 +18,55 @@
             BatchNorm = SynchronizedBatchNorm2d
         else:
             BatchNorm = nn.BatchNorm2d
+        
+        self.backbone1 = build_backbone(backbone, output_stride, BatchNorm, input_dim=input_dim, pretrained=pretrained) # RGB
+        self.aspp1 = build_aspp(backbone, output_stride, BatchNorm)
+        self.backbone2 = build_backbone(backbone, output_stride, BatchNorm, input_dim=2) # aolp
+        self.aspp2 = build_aspp(backbone, output_stride, BatchNorm)
+        self.backbone3 = build_backbone(backbone, output_stride, BatchNorm, input_dim=1) # dolp
+        self.aspp3 = build_aspp(backbone, output_stride, BatchNorm)
+        self.backbone4 = build_backbone(backbone, output_stride, BatchNorm, input_dim=1) # nir
+        self.aspp4 = build_aspp(backbone, output_stride, BatchNorm)
 
-        self.backbone = build_backbone(backbone, output_stride, BatchNorm)
-        self.aspp = build_aspp(backbone, output_stride, BatchNorm)
-        self.decoder = build_decoder(num_classes, backbone, BatchNorm)
+        self.decoder = build_decoder(num_classes, backbone, BatchNorm, ratio, input_heads=4)
 
         self.freeze_bn = freeze_bn
+    def forward(self, input1, input2=None, input3=None, input4=None, mask=None):
+
+        x1, low_level_feat1 = self.backbone1(input1)
+        x1 = self.aspp1(x1)
+        # AoLP
+        if input2 is not None:
+            x2, low_level_feat2 = self.backbone2(input2)
+            #x2=x2[0]
+            x2 = self.aspp2(x2)
+        else:
+            x2 = torch.zeros_like(x1)
+            low_level_feat2 = torch.zeros_like(low_level_feat1)
+        # DoLP
+        if input3 is not None:
+            x3, low_level_feat3 = self.backbone3(input3)
+            #x3=x3[0]
+            x3 = self.aspp3(x3)
+        else:
+            x3 = torch.zeros_like(x1)
+            low_level_feat3 = torch.zeros_like(low_level_feat1)
+        # NIR
+        if input4 is not None:
+            x4, low_level_feat4 = self.backbone4(input4)
+            #x4=x4[0]
+            x4 = self.aspp4(x4)
+        else:
+            x4 = torch.zeros_like(x1)
+            low_level_feat4 = torch.zeros_like(low_level_feat1)
+
+        x = torch.cat([x1,x2,x3,x4],dim=1) 
+
+        low_level_feat = torch.cat([low_level_feat1,low_level_feat2,low_level_feat3,low_level_feat4],dim=1)
+        x = self.decoder(x, low_level_feat, mask)
+
+        x = F.interpolate(x, size=input1.size()[2:], mode='bilinear', align_corners=True)
 
-    def forward(self, input):
-        x, low_level_feat = self.backbone(input)
-        x = self.aspp(x)
-        x = self.decoder(x, low_level_feat)
-        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)
 
         return x
 
@@ -40,36 +78,71 @@
                 m.eval()
 
     def get_1x_lr_params(self):
-        modules = [self.backbone]
+        modules = [self.backbone1,self.backbone2,self.backbone3,self.backbone4]
         for i in range(len(modules)):
             for m in modules[i].named_modules():
                 if self.freeze_bn:
                     if isinstance(m[1], nn.Conv2d):
                         for p in m[1].parameters():
                             if p.requires_grad:
+                                #print(p)
                                 yield p
                 else:
                     if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \
                             or isinstance(m[1], nn.BatchNorm2d):
                         for p in m[1].parameters():
                             if p.requires_grad:
+                               # print(p)
                                 yield p
 
     def get_10x_lr_params(self):
-        modules = [self.aspp, self.decoder]
+        modules = [self.aspp1, self.aspp2, self.aspp3, self.aspp4, self.decoder]
         for i in range(len(modules)):
             for m in modules[i].named_modules():
+
                 if self.freeze_bn:
                     if isinstance(m[1], nn.Conv2d):
                         for p in m[1].parameters():
                             if p.requires_grad:
+                                
                                 yield p
                 else:
+            
                     if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \
-                            or isinstance(m[1], nn.BatchNorm2d):
+                            or isinstance(m[1], nn.BatchNorm2d) or isinstance(m[1],nn.Linear):
                         for p in m[1].parameters():
+                            #if m[0].split('.')[0]=='condconv':
+                                #continue
                             if p.requires_grad:
+                                #print(m[0])
                                 yield p
+                    if m[0]=='gamma':
+                        for p in m[1].parameters():
+                            if p.requires_grad:
+                                yield p
+    '''
+    def get_100x_lr_params(self):
+        modules = [self.decoder]
+        for i in range(len(modules)):
+            for m in modules[i].named_modules():
+
+                if self.freeze_bn:
+                    if isinstance(m[1], nn.Conv2d):
+                        for p in m[1].parameters():
+                            if p.requires_grad:
+                                #print(m[0])
+                                yield p
+                else:
+                    if m[0].split('.')[0]=='condconv':
+                        for p in m[1].parameters():
+                            if p.requires_grad:
+                                #print(m[0])
+                                yield p
+        #for  m, parameters in modules[4].named_parameters():
+            #for p in parameters:
+                #if p.requires_grad:
+                    #yield p
+    '''
 
 if __name__ == "__main__":
     model = DeepLab(backbone='mobilenet', output_stride=16)
Binary files pytorch-deeplab-xception/modeling/sync_batchnorm/__pycache__/__init__.cpython-36.pyc and public-multimodal-material-segmentation/modeling/sync_batchnorm/__pycache__/__init__.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/sync_batchnorm/__pycache__/__init__.cpython-38.pyc and public-multimodal-material-segmentation/modeling/sync_batchnorm/__pycache__/__init__.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/modeling/sync_batchnorm/__pycache__/batchnorm.cpython-36.pyc and public-multimodal-material-segmentation/modeling/sync_batchnorm/__pycache__/batchnorm.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/sync_batchnorm/__pycache__/batchnorm.cpython-38.pyc and public-multimodal-material-segmentation/modeling/sync_batchnorm/__pycache__/batchnorm.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/modeling/sync_batchnorm/__pycache__/comm.cpython-36.pyc and public-multimodal-material-segmentation/modeling/sync_batchnorm/__pycache__/comm.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/sync_batchnorm/__pycache__/comm.cpython-38.pyc and public-multimodal-material-segmentation/modeling/sync_batchnorm/__pycache__/comm.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/modeling/sync_batchnorm/__pycache__/replicate.cpython-36.pyc and public-multimodal-material-segmentation/modeling/sync_batchnorm/__pycache__/replicate.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/modeling/sync_batchnorm/__pycache__/replicate.cpython-38.pyc and public-multimodal-material-segmentation/modeling/sync_batchnorm/__pycache__/replicate.cpython-38.pyc differ
diff -rdNuB -x .git pytorch-deeplab-xception/mypath.py public-multimodal-material-segmentation/mypath.py
--- pytorch-deeplab-xception/mypath.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/mypath.py	2022-03-24 20:37:30.252648062 +0900
@@ -2,7 +2,19 @@
     @staticmethod
     def db_root_dir(dataset):
         if dataset == 'pascal':
-            return '/path/to/datasets/VOCdevkit/VOC2012/'  # folder that contains VOCdevkit/.
+            return './datasets/VOCdevkit/VOC2012/'  # folder that contains VOCdevkit/.
+        elif dataset == 'kitti':
+            return './datasets/KITTI_material_dataset.old/'  # folder that contains KITTI_material_dataset.
+        elif dataset == 'kitti_advanced':
+            return './datasets/KITTI_material/'  # folder that contains KITTI_material_dataset.
+        elif dataset == 'kitti_advanced_manta':
+            return '/home/wakaki/manta_local/KITTI_material/'  # folder that contains KITTI_material_dataset.
+        elif dataset == 'handmade_dataset':
+            return './datasets/handmade_dataset_for_train/'  # folder that contains KITTI_material_dataset.
+        elif dataset == 'handmade_dataset_stereo':
+            return './datasets/handmade_dataset_for_train_stereo/'  # folder that contains KITTI_material_dataset.
+        elif dataset == 'multimodal_dataset':
+            return './datasets/multimodal_dataset/'  # folder that contains KITTI_material_dataset.
         elif dataset == 'sbd':
             return '/path/to/datasets/benchmark_RELEASE/'  # folder that contains dataset/.
         elif dataset == 'cityscapes':
diff -rdNuB -x .git pytorch-deeplab-xception/requirements.txt public-multimodal-material-segmentation/requirements.txt
--- pytorch-deeplab-xception/requirements.txt	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/requirements.txt	2022-03-24 20:37:30.252648062 +0900
@@ -0,0 +1,53 @@
+absl-py==0.10.0
+aiohttp==3.6.2
+astroid==2.4.2
+async-timeout==3.0.1
+attrs==20.2.0
+cachetools==4.1.1
+certifi==2020.6.20
+chardet==3.0.4
+cycler==0.10.0
+Cython==0.29.21
+future==0.18.2
+google-auth==1.22.0
+google-auth-oauthlib==0.4.1
+grpcio==1.32.0
+idna==2.10
+idna-ssl==1.1.0
+importlib-metadata==2.0.0
+isort==5.5.4
+kiwisolver==1.2.0
+lazy-object-proxy==1.4.3
+Markdown==3.2.2
+matplotlib==3.3.2
+mccabe==0.6.1
+multidict==4.7.6
+numpy==1.19.2
+oauthlib==3.1.0
+opencv-python==4.4.0.44
+Pillow==7.2.0
+protobuf==3.13.0
+pyasn1==0.4.8
+pyasn1-modules==0.2.8
+pycocotools==2.0.2
+pylint==2.6.0
+pyparsing==2.4.7
+python-dateutil==2.8.1
+requests==2.24.0
+requests-oauthlib==1.3.0
+rsa==4.6
+scipy==1.5.2
+shape-commentator==0.7.0
+six==1.15.0
+tensorboard==2.3.0
+tensorboard-plugin-wit==1.7.0
+tensorboardX==2.1
+toml==0.10.1
+tqdm==4.19.9
+typed-ast==1.4.1
+typing-extensions==3.7.4.3
+urllib3==1.25.10
+Werkzeug==1.0.1
+wrapt==1.12.1
+yarl==1.6.0
+zipp==3.2.0
diff -rdNuB -x .git pytorch-deeplab-xception/test.py public-multimodal-material-segmentation/test.py
--- pytorch-deeplab-xception/test.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/test.py	2022-03-24 20:37:30.252648062 +0900
@@ -0,0 +1,371 @@
+import argparse
+import os
+import numpy as np
+from tqdm import tqdm
+import random
+import matplotlib.pyplot as plt
+
+from mypath import Path
+from dataloaders import make_data_loader
+from modeling.sync_batchnorm.replicate import patch_replication_callback
+from modeling.deeplab import *
+from utils.loss import SegmentationLosses
+from utils.calculate_weights import calculate_weigths_labels
+from utils.lr_scheduler import LR_Scheduler
+from utils.saver import Saver
+from utils.summaries import TensorboardSummary
+from utils.metrics import Evaluator
+
+
+
+LABEL_COLORS_NEW_JP = {
+    "#2ca02c" : "アスファルト",      #0
+    "#1f77b4" : "コンクリート",     #1
+    "#ff7f0e" : "金属",        #2
+    "#d62728" : "白線", #3
+    "#8c564b" : "布",#4
+    "#7f7f7f" : "ガラス",        #5
+    "#bcbd22" : "セメント",      #6
+    "#ff9896" : "プラスチック",      #7
+    "#17becf" : "ゴム",#8
+    "#aec7e8" : "砂・土",         #9
+    "#c49c94" : "砂利",       #10
+    "#c5b0d5" : "陶器",      #11
+    "#f7b6d2" : "石",  #12
+    "#c7c7c7" : "レンガ",        #13
+    "#dbdb8d" : "草",        #14
+    "#9edae5" : "木",         #15
+    "#393b79" : "葉",         #16
+    "#6b6ecf" : "水",        #17
+    "#9c9ede" : "人体",   #18
+    "#637939" : "空"}          #19
+
+LABEL_COLORS_NEW_EN = {
+    "#2ca02c" : "asphalt",      #0
+    "#1f77b4" : "concrete",     #1
+    "#ff7f0e" : "metal",        #2
+    "#d62728" : "road marking", #3
+    "#8c564b" : "fabric, leather",#4
+    "#7f7f7f" : "glass",        #5
+    "#bcbd22" : "plaster",      #6
+    "#ff9896" : "plastic",      #7
+    "#17becf" : "rubber",#8
+    "#aec7e8" : "sand",         #9
+    "#c49c94" : "gravel",       #10
+    "#c5b0d5" : "ceramic",      #11
+    "#f7b6d2" : "cobblestone",  #12
+    "#c7c7c7" : "brick",        #13
+    "#dbdb8d" : "grass",        #14
+    "#9edae5" : "wood",         #15
+    "#393b79" : "leaf",         #16
+    "#6b6ecf" : "water",        #17
+    "#9c9ede" : "human body",   #18
+    "#637939" : "sky"}          #19
+
+
+        
+
+        
+       
+        
+class TesterMultimodal(object):
+    def __init__(self, args):
+        self.args = args
+
+        # Define Tensorboard Summary
+        self.summary = TensorboardSummary(f'{os.path.dirname(args.pth_path)}/test')
+        self.writer = self.summary.create_summary()
+        
+        # Define Dataloader
+        kwargs = {'num_workers': args.workers, 'pin_memory': True}
+        self.train_loader, self.val_loader, self.test_loader, self.nclass = make_data_loader(args, **kwargs)
+
+        # Define network
+        input_dim = 3
+        
+        model = DeepLabMultiInput(num_classes=self.nclass,
+                        backbone=args.backbone,
+                        output_stride=args.out_stride,
+                        sync_bn=args.sync_bn,
+                        freeze_bn=args.freeze_bn,
+                        input_dim=input_dim,
+                        ratio=args.ratio,
+                        pretrained=args.use_pretrained_resnet)
+        
+        train_params = [{'params': model.get_1x_lr_params(), 'lr': args.lr},
+                        {'params': model.get_10x_lr_params(), 'lr': args.lr * 10}]
+        
+        # Define Optimizer
+        optimizer = torch.optim.SGD(model.parameters(), momentum=args.momentum,lr=args.lr,
+                                    weight_decay=args.weight_decay, nesterov=args.nesterov)
+
+        # Define Criterion
+        # whether to use class balanced weights
+        if args.use_balanced_weights:
+            classes_weights_path = os.path.join(Path.db_root_dir(args.dataset), args.dataset+'_classes_weights.npy')
+            if os.path.isfile(classes_weights_path):
+                weight = np.load(classes_weights_path)
+            else:
+                weight = calculate_weigths_labels(args.dataset, self.train_loader, self.nclass)
+            weight = torch.from_numpy(weight.astype(np.float32))
+        else:
+            weight = None
+        # self.criterion = SegmentationLosses(weight=weight, cuda=args.cuda, ignore_index=0).build_loss(mode=args.loss_type)
+        self.criterion = SegmentationLosses(weight=weight, cuda=args.cuda).build_loss(mode=args.loss_type)
+        self.model, self.optimizer = model, optimizer
+
+        # Load model parameters
+        checkpoint = torch.load(args.pth_path)
+        self.model.load_state_dict(checkpoint['state_dict'])
+        
+        # Define Evaluator
+        self.evaluator = Evaluator(self.nclass)
+        # Define lr scheduler
+        self.scheduler = LR_Scheduler(args.lr_scheduler, args.lr,
+                                            args.epochs, len(self.train_loader))
+
+        # Using cuda
+        if args.cuda:
+            self.model = torch.nn.DataParallel(self.model, device_ids=self.args.gpu_ids)
+            patch_replication_callback(self.model)
+            self.model = self.model.cuda()
+
+        # Resuming checkpoint
+        self.best_pred = 0.0
+        if args.resume is not None:
+            if not os.path.isfile(args.resume):
+                raise RuntimeError("=> no checkpoint found at '{}'" .format(args.resume))
+            checkpoint = torch.load(args.resume)
+            args.start_epoch = checkpoint['epoch']
+            print(checkpoint['epoch'])
+            if args.cuda:
+                self.model.module.load_state_dict(checkpoint['state_dict'])
+            else:
+                self.model.load_state_dict(checkpoint['state_dict'])
+            if not args.ft:
+                self.optimizer.load_state_dict(checkpoint['optimizer'])
+            self.best_pred = checkpoint['best_pred']
+            print("=> loaded checkpoint '{}' (epoch {})"
+                  .format(args.resume, checkpoint['epoch']))
+
+        # Clear start epoch if fine-tuning
+        if args.ft:
+            args.start_epoch = 0
+
+    def test(self, epoch=0):
+        self.model.eval()
+        self.evaluator.reset()
+        tbar = tqdm(self.test_loader, desc='\r')
+        test_loss = 0.0
+        scaler = torch.cuda.amp.GradScaler()
+        image_all = None
+        target_all = None
+        output_all = None
+        for i, sample in enumerate(tbar):
+            image, target, aolp, dolp, nir, nir_mask, uvmap,mask = \
+                sample['image'], sample['label'], sample['aolp'], sample['dolp'], sample['nir'], sample['nir_mask'], sample['uvmap'],sample['mask']
+            if self.args.cuda:
+                image, target, aolp, dolp, nir, nir_mask, uvmap,mask = image.cuda(), target.cuda(), aolp.cuda(), dolp.cuda(), nir.cuda(), nir_mask.cuda(), uvmap.cuda(),mask.cuda()
+            aolp = aolp if self.args.use_aolp else None
+            dolp = dolp if self.args.use_dolp else None
+            nir  = nir  if self.args.use_nir else None
+            nir_mask = nir_mask  if self.args.use_nir else None
+
+            
+            with torch.cuda.amp.autocast():
+                with torch.no_grad():
+                    output= self.model(image, aolp, dolp, nir, mask)
+                loss = self.criterion(output, target, nir_mask)
+            test_loss += loss.item()
+            tbar.set_description('Test loss: %.3f' % (test_loss / (i + 1)))
+            
+            pred = output.data.cpu().numpy()
+            target_ = target.cpu().numpy()
+            pred = np.argmax(pred, axis=1)
+            if image_all is None:
+                image_all  =  image.cpu().clone()
+                target_all = target.cpu().clone()
+                output_all = output.cpu().clone()
+            else:
+                image_all  = torch.cat(( image_all, image.cpu().clone()),dim=0)
+                target_all = torch.cat((target_all,target.cpu().clone()),dim=0)
+                output_all = torch.cat((output_all,output.cpu().clone()),dim=0)
+                
+            # Add batch sample into evaluator
+            self.evaluator.add_batch(target_, pred)
+        # Fast test during the training
+        Acc = self.evaluator.Pixel_Accuracy()
+        Acc_class = self.evaluator.Pixel_Accuracy_Class()
+        mIoU = self.evaluator.Mean_Intersection_over_Union()
+        FWIoU = self.evaluator.Frequency_Weighted_Intersection_over_Union()
+        confusion_matrix = self.evaluator.confusion_matrix
+        np.save(f'{os.path.dirname(args.pth_path)}/test/confusion_matrix.npy',confusion_matrix)
+
+        self.writer.add_scalar('test/mIoU', mIoU, epoch)
+        self.writer.add_scalar('test/Acc', Acc, epoch)
+        self.writer.add_scalar('test/Acc_class', Acc_class, epoch)
+        self.writer.add_scalar('test/fwIoU', FWIoU, epoch)
+        self.summary.visualize_test_image(self.writer, self.args.dataset, image_all, target_all, output_all, 0)
+        
+        print('Test:')
+        print('[Epoch: %d, numImages: %5d]' % (epoch, i * self.args.batch_size + image.data.shape[0]))
+        print("Acc:{}, Acc_class:{}, mIoU:{}, fwIoU: {}".format(Acc, Acc_class, mIoU, FWIoU))
+        #print('Loss: %.3f' % test_loss)
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser(description="PyTorch DeeplabV3Plus Training")
+    parser.add_argument('--backbone', type=str, default='resnet',
+                        choices=['resnet', 'xception', 'drn', 'mobilenet', 'resnet_adv', 'xception_adv','resnet_condconv'],
+                        help='backbone name (default: resnet)')
+    parser.add_argument('--out-stride', type=int, default=16,
+                        help='network output stride (default: 8)')
+    parser.add_argument('--dataset', type=str, default='pascal',
+                        choices=['pascal', 'coco', 'cityscapes', 'kitti', 'kitti_advanced', 'kitti_advanced_manta', 'handmade_dataset', 'handmade_dataset_stereo', 'multimodal_dataset'],
+                        help='dataset name (default: pascal)')
+    parser.add_argument('--use-sbd', action='store_true', default=False,
+                        help='whether to use SBD dataset (default: True)')
+    parser.add_argument('--workers', type=int, default=4,
+                        metavar='N', help='dataloader threads')
+    parser.add_argument('--base-size', type=int, default=513,
+                        help='base image size')
+    parser.add_argument('--crop-size', type=int, default=513,
+                        help='crop image size')
+    parser.add_argument('--sync-bn', type=bool, default=None,
+                        help='whether to use sync bn (default: auto)')
+    parser.add_argument('--freeze-bn', type=bool, default=False,
+                        help='whether to freeze bn parameters (default: False)')
+    parser.add_argument('--loss-type', type=str, default='ce',
+                        choices=['ce', 'focal', 'original'],
+                        help='loss func type (default: ce)')
+    # training hyper params
+    parser.add_argument('--epochs', type=int, default=None, metavar='N',
+                        help='number of epochs to train (default: auto)')
+    parser.add_argument('--start_epoch', type=int, default=0,
+                        metavar='N', help='start epochs (default:0)')
+    parser.add_argument('--batch-size', type=int, default=None,
+                        metavar='N', help='input batch size for \
+                                training (default: auto)')
+    parser.add_argument('--test-batch-size', type=int, default=None,
+                        metavar='N', help='input batch size for \
+                                testing (default: auto)')
+    parser.add_argument('--use-balanced-weights', action='store_true', default=True,
+                        help='whether to use balanced weights (default: False)')
+    parser.add_argument('--ratio', type=float, default=None, metavar='N',
+                        help='number of ratio in RGFSConv (default: 1)')
+    # optimizer params
+    parser.add_argument('--lr', type=float, default=None, metavar='LR',
+                        help='learning rate (default: auto)')
+    parser.add_argument('--lr-scheduler', type=str, default='poly',
+                        choices=['poly', 'step', 'cos'],
+                        help='lr scheduler mode: (default: poly)')
+    parser.add_argument('--momentum', type=float, default=0.9,
+                        metavar='M', help='momentum (default: 0.9)')
+    parser.add_argument('--weight-decay', type=float, default=5e-4,
+                        metavar='M', help='w-decay (default: 5e-4)')
+    parser.add_argument('--nesterov', action='store_true', default=False,
+                        help='whether use nesterov (default: False)')
+    # cuda, seed and logging
+    parser.add_argument('--no-cuda', action='store_true', default=
+                        False, help='disables CUDA training')
+    parser.add_argument('--gpu-ids', type=str, default='0',
+                        help='use which gpu to train, must be a \
+                        comma-separated list of integers only (default=0)')
+    parser.add_argument('--seed', type=int, default=1, metavar='S',
+                        help='random seed (default: 1)')
+    # checking point
+    parser.add_argument('--resume', type=str, default=None,
+                        help='put the path to resuming file if needed')
+    parser.add_argument('--checkname', type=str, default=None,
+                        help='set the checkpoint name')
+    # finetuning pre-trained models
+    parser.add_argument('--ft', action='store_true', default=False,
+                        help='finetuning on a different dataset')
+    # evaluation option
+    parser.add_argument('--eval-interval', type=int, default=1,
+                        help='evaluuation interval (default: 1)')
+    parser.add_argument('--no-val', action='store_true', default=False,
+                        help='skip validation during training')
+
+    # propagation and positional encoding option
+    parser.add_argument('--propagation', type=int, default=0,
+                        help='image propagation length (default: 0)')
+    parser.add_argument('--positional-encoding', action='store_true', default=False,
+                        help='use positional encoding')
+    parser.add_argument('--use-aolp', action='store_true', default=False,
+                        help='use aolp')
+    parser.add_argument('--use-dolp', action='store_true', default=False,
+                        help='use dolp')
+    parser.add_argument('--use-nir', action='store_true', default=False,
+                        help='use nir')
+    parser.add_argument('--use-pretrained-resnet', action='store_true', default=False,
+                        help='use pretrained resnet101')
+    parser.add_argument('--list-folder', type=str, default='list_folder1')
+    parser.add_argument('--is-multimodal', action='store_true', default=False,
+                        help='use multihead architecture')
+    parser.add_argument('--pth-path', type=str, default=None,
+                        help='set the pth file path')
+
+    args = parser.parse_args()
+    args.cuda = not args.no_cuda and torch.cuda.is_available()
+    if args.cuda:
+        try:
+            args.gpu_ids = [int(s) for s in args.gpu_ids.split(',')]
+        except ValueError:
+            raise ValueError('Argument --gpu_ids must be a comma-separated list of integers only')
+
+    if args.sync_bn is None:
+        if args.cuda and len(args.gpu_ids) > 1:
+            args.sync_bn = True
+        else:
+            args.sync_bn = False
+
+    # default settings for epochs, batch_size and lr
+    if args.epochs is None:
+        epoches = {
+            'coco': 30,
+            'cityscapes': 200,
+            'pascal': 50,
+            'kitti': 50,
+            'kitti_advanced': 50
+        }
+        args.epochs = epoches[args.dataset.lower()]
+
+    if args.batch_size is None:
+        args.batch_size = 4 * len(args.gpu_ids)
+
+    if args.test_batch_size is None:
+        args.test_batch_size = args.batch_size
+
+    if args.lr is None:
+        lrs = {
+            'coco': 0.1,
+            'cityscapes': 0.01,
+            'pascal': 0.007,
+            'kitti' : 0.01,
+            'kitti_advanced' : 0.01
+        }
+        args.lr = lrs[args.dataset.lower()] / (4 * len(args.gpu_ids)) * args.batch_size
+
+    if args.checkname is None:
+        args.checkname = 'deeplab-'+str(args.backbone)
+    print(args)
+    # input('Check arguments! Press Enter...')
+    # os.environ['PYTHONHASHSEED'] = str(args.seed)
+    # random.seed(args.seed)
+    # np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    # torch.cuda.manual_seed(args.seed)
+    # torch.cuda.manual_seed_all(args.seed)
+    # torch.backends.cudnn.deterministic = True
+    # torch.backends.cudnn.benchmark = False
+
+    if args.is_multimodal:
+        print("USE Multimodal Model")
+        tester = TesterMultimodal(args)
+    else:
+        tester = TesterAdv(args)
+    print('Starting Epoch:', tester.args.start_epoch)
+    print('Total Epoches:', tester.args.epochs)
+    tester.test()
+    tester.writer.close()
+    print(args)
diff -rdNuB -x .git pytorch-deeplab-xception/train.py public-multimodal-material-segmentation/train.py
--- pytorch-deeplab-xception/train.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/train.py	2022-03-24 20:37:30.252648062 +0900
@@ -2,7 +2,8 @@
 import os
 import numpy as np
 from tqdm import tqdm
-
+import random
+from modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d
 from mypath import Path
 from dataloaders import make_data_loader
 from modeling.sync_batchnorm.replicate import patch_replication_callback
@@ -14,7 +15,8 @@
 from utils.summaries import TensorboardSummary
 from utils.metrics import Evaluator
 
-class Trainer(object):
+
+class TrainerMultimodal(object):
     def __init__(self, args):
         self.args = args
 
@@ -30,18 +32,23 @@
         self.train_loader, self.val_loader, self.test_loader, self.nclass = make_data_loader(args, **kwargs)
 
         # Define network
-        model = DeepLab(num_classes=self.nclass,
+        input_dim = 3
+        
+        model = DeepLabMultiInput(num_classes=self.nclass,
                         backbone=args.backbone,
                         output_stride=args.out_stride,
                         sync_bn=args.sync_bn,
-                        freeze_bn=args.freeze_bn)
+                        freeze_bn=args.freeze_bn,
+                        input_dim=input_dim,
+                        ratio=args.ratio,
+                        pretrained=args.use_pretrained_resnet)
+        
 
         train_params = [{'params': model.get_1x_lr_params(), 'lr': args.lr},
-                        {'params': model.get_10x_lr_params(), 'lr': args.lr * 10}]
-
+                        {'params': model.get_10x_lr_params(), 'lr': args.lr*10}]
+        
         # Define Optimizer
-        optimizer = torch.optim.SGD(train_params, momentum=args.momentum,
-                                    weight_decay=args.weight_decay, nesterov=args.nesterov)
+        optimizer = torch.optim.SGD(train_params,momentum=args.momentum, nesterov=args.nesterov)
 
         # Define Criterion
         # whether to use class balanced weights
@@ -54,6 +61,7 @@
             weight = torch.from_numpy(weight.astype(np.float32))
         else:
             weight = None
+        # self.criterion = SegmentationLosses(weight=weight, cuda=args.cuda, ignore_index=0).build_loss(mode=args.loss_type)
         self.criterion = SegmentationLosses(weight=weight, cuda=args.cuda).build_loss(mode=args.loss_type)
         self.model, self.optimizer = model, optimizer
         
@@ -65,9 +73,11 @@
 
         # Using cuda
         if args.cuda:
+            self.model = self.model.cuda()
             self.model = torch.nn.DataParallel(self.model, device_ids=self.args.gpu_ids)
+            #self.mask_model = torch.nn.DataParallel(self.mask_model, device_ids=self.args.gpu_ids)
             patch_replication_callback(self.model)
-            self.model = self.model.cuda()
+            
 
         # Resuming checkpoint
         self.best_pred = 0.0
@@ -89,22 +99,50 @@
         # Clear start epoch if fine-tuning
         if args.ft:
             args.start_epoch = 0
-
+    
     def training(self, epoch):
         train_loss = 0.0
         self.model.train()
         tbar = tqdm(self.train_loader)
         num_img_tr = len(self.train_loader)
+        scaler = torch.cuda.amp.GradScaler()
         for i, sample in enumerate(tbar):
-            image, target = sample['image'], sample['label']
+            image, target, aolp, dolp, nir, nir_mask, uvmap,mask = \
+                sample['image'], sample['label'], sample['aolp'], sample['dolp'], sample['nir'], sample['nir_mask'], sample['uvmap'],sample['mask']
+
+            # # check tensors            
+            # import matplotlib.pyplot as plt
+            # import sys
+            # img_np = image[0,0].numpy()
+            # target_np = target[0].numpy()
+            # aolp_np = np.arctan2(aolp[0,0].numpy(),aolp[0,1].numpy())
+            # dolp_np = dolp[0,0].numpy()
+            # nir_np = nir[0,0].numpy()
+            # print('saveing')
+            # plt.imsave('np_img.png',img_np)
+            # plt.imsave('np_target.png',target_np)
+            # plt.imsave('np_aolp.png',aolp_np)
+            # plt.imsave('np_dolp.png',dolp_np)
+            # plt.imsave('np_nir.png',nir_np)
+            # print('done')
+            # input('press enter')
+            # continue
+
             if self.args.cuda:
-                image, target = image.cuda(), target.cuda()
+                image, target, aolp, dolp, nir, nir_mask, uvmap,mask = image.cuda(), target.cuda(), aolp.cuda(), dolp.cuda(), nir.cuda(), nir_mask.cuda(), uvmap.cuda(),mask.cuda()
             self.scheduler(self.optimizer, i, epoch, self.best_pred)
             self.optimizer.zero_grad()
-            output = self.model(image)
-            loss = self.criterion(output, target)
-            loss.backward()
-            self.optimizer.step()
+            aolp = aolp if self.args.use_aolp else None
+            dolp = dolp if self.args.use_dolp else None
+            nir  = nir  if self.args.use_nir else None
+            nir_mask = nir_mask  if self.args.use_nir else None            
+            
+            with torch.cuda.amp.autocast():
+                output = self.model(image, aolp, dolp, nir, mask)
+                loss = self.criterion(output, target, nir_mask)
+            scaler.scale(loss).backward()
+            scaler.step(self.optimizer)
+            scaler.update()
             train_loss += loss.item()
             tbar.set_description('Train loss: %.3f' % (train_loss / (i + 1)))
             self.writer.add_scalar('train/total_loss_iter', loss.item(), i + num_img_tr * epoch)
@@ -112,10 +150,10 @@
             # Show 10 * 3 inference results each epoch
             if i % (num_img_tr // 10) == 0:
                 global_step = i + num_img_tr * epoch
-                self.summary.visualize_image(self.writer, self.args.dataset, image, target, output, global_step)
-
+                self.summary.visualize_image(self.writer, self.args.dataset, image[0], target, output, global_step)
+                
         self.writer.add_scalar('train/total_loss_epoch', train_loss, epoch)
-        print('[Epoch: %d, numImages: %5d]' % (epoch, i * self.args.batch_size + image.data.shape[0]))
+        print('[Epoch: %d, numImages: %5d]' % (epoch, i * self.args.batch_size + image[0].data.shape[0]))
         print('Loss: %.3f' % train_loss)
 
         if self.args.no_val:
@@ -134,20 +172,29 @@
         self.evaluator.reset()
         tbar = tqdm(self.val_loader, desc='\r')
         test_loss = 0.0
+        scaler = torch.cuda.amp.GradScaler()
         for i, sample in enumerate(tbar):
-            image, target = sample['image'], sample['label']
+            image, target, aolp, dolp, nir, nir_mask, uvmap,mask = \
+                sample['image'], sample['label'], sample['aolp'], sample['dolp'], sample['nir'], sample['nir_mask'], sample['uvmap'],sample['mask']
             if self.args.cuda:
-                image, target = image.cuda(), target.cuda()
-            with torch.no_grad():
-                output = self.model(image)
-            loss = self.criterion(output, target)
+                image, target, aolp, dolp, nir, nir_mask, uvmap,mask = image.cuda(), target.cuda(), aolp.cuda(), dolp.cuda(), nir.cuda(), nir_mask.cuda(), uvmap.cuda(),mask.cuda()
+            aolp = aolp if self.args.use_aolp else None
+            dolp = dolp if self.args.use_dolp else None
+            nir  = nir  if self.args.use_nir else None
+            nir_mask = nir_mask  if self.args.use_nir else None         
+            
+            with torch.cuda.amp.autocast():
+                with torch.no_grad():
+                    output = self.model(image, aolp, dolp, nir, mask)
+                    
+                loss = self.criterion(output, target, nir_mask)
             test_loss += loss.item()
-            tbar.set_description('Test loss: %.3f' % (test_loss / (i + 1)))
+            tbar.set_description('Val loss: %.3f' % (test_loss / (i + 1)))
             pred = output.data.cpu().numpy()
-            target = target.cpu().numpy()
+            target_ = target.cpu().numpy()
             pred = np.argmax(pred, axis=1)
             # Add batch sample into evaluator
-            self.evaluator.add_batch(target, pred)
+            self.evaluator.add_batch(target_, pred)
 
         # Fast test during the training
         Acc = self.evaluator.Pixel_Accuracy()
@@ -159,13 +206,19 @@
         self.writer.add_scalar('val/Acc', Acc, epoch)
         self.writer.add_scalar('val/Acc_class', Acc_class, epoch)
         self.writer.add_scalar('val/fwIoU', FWIoU, epoch)
+
+        global_step = epoch
+        self.summary.visualize_validation_image(self.writer, self.args.dataset, image[0], target, output, global_step)
+        
         print('Validation:')
-        print('[Epoch: %d, numImages: %5d]' % (epoch, i * self.args.batch_size + image.data.shape[0]))
+        print('[Epoch: %d, numImages: %5d]' % (epoch, i * self.args.batch_size + image[0].data.shape[0]))
         print("Acc:{}, Acc_class:{}, mIoU:{}, fwIoU: {}".format(Acc, Acc_class, mIoU, FWIoU))
         print('Loss: %.3f' % test_loss)
 
         new_pred = mIoU
         if new_pred > self.best_pred:
+            if True:
+                self.test(epoch)
             is_best = True
             self.best_pred = new_pred
             self.saver.save_checkpoint({
@@ -175,30 +228,82 @@
                 'best_pred': self.best_pred,
             }, is_best)
 
-def main():
+    def test(self, epoch):
+        self.model.eval()
+        self.evaluator.reset()
+        tbar = tqdm(self.test_loader, desc='\r')
+        test_loss = 0.0
+        scaler = torch.cuda.amp.GradScaler()
+        output_all = None
+        for i, sample in enumerate(tbar):
+            image, target, aolp, dolp, nir, nir_mask, uvmap,mask = \
+                sample['image'], sample['label'], sample['aolp'], sample['dolp'], sample['nir'], sample['nir_mask'], sample['uvmap'],sample['mask']
+            if self.args.cuda:
+                image, target, aolp, dolp, nir, nir_mask, uvmap,mask = image.cuda(), target.cuda(), aolp.cuda(), dolp.cuda(), nir.cuda(), nir_mask.cuda(), uvmap.cuda(),mask.cuda()
+            aolp = aolp if self.args.use_aolp else None
+            dolp = dolp if self.args.use_dolp else None
+            nir  = nir  if self.args.use_nir else None
+            nir_mask = nir_mask  if self.args.use_nir else None            
+            
+            with torch.cuda.amp.autocast():
+                with torch.no_grad():
+                    output = self.model(image, aolp, dolp, nir, mask)
+                loss = self.criterion(output, target, nir_mask)
+            test_loss += loss.item()
+            tbar.set_description('Test loss: %.3f' % (test_loss / (i + 1)))
+            pred = output.data.cpu().numpy()
+            target_ = target.cpu().numpy()
+            pred = np.argmax(pred, axis=1)
+            if output_all is None:
+                output_all = output.cpu().clone()
+            else:
+                output_all = torch.cat((output_all,output.cpu().clone()),dim=0)
+            # Add batch sample into evaluator
+            self.evaluator.add_batch(target_, pred)
+
+        # Fast test during the training
+        Acc = self.evaluator.Pixel_Accuracy()
+        Acc_class = self.evaluator.Pixel_Accuracy_Class()
+        mIoU = self.evaluator.Mean_Intersection_over_Union()
+        FWIoU = self.evaluator.Frequency_Weighted_Intersection_over_Union()
+        self.writer.add_scalar('test/total_loss_epoch', test_loss, epoch)
+        self.writer.add_scalar('test/mIoU', mIoU, epoch)
+        self.writer.add_scalar('test/Acc', Acc, epoch)
+        self.writer.add_scalar('test/Acc_class', Acc_class, epoch)
+        self.writer.add_scalar('test/fwIoU', FWIoU, epoch)
+
+        global_step = epoch
+        self.summary.visualize_test_image(self.writer, self.args.dataset, image[0], target, output, global_step)
+        
+        print('Test:')
+        print('[Epoch: %d, numImages: %5d]' % (epoch, i * self.args.batch_size + image[0].data.shape[0]))
+        print("Acc:{}, Acc_class:{}, mIoU:{}, fwIoU: {}".format(Acc, Acc_class, mIoU, FWIoU))
+        print('Loss: %.3f' % test_loss)
+
+if __name__ == "__main__":
     parser = argparse.ArgumentParser(description="PyTorch DeeplabV3Plus Training")
     parser.add_argument('--backbone', type=str, default='resnet',
-                        choices=['resnet', 'xception', 'drn', 'mobilenet'],
+                        choices=['resnet', 'xception', 'drn', 'mobilenet', 'resnet_adv', 'xception_adv','resnet_condconv'],
                         help='backbone name (default: resnet)')
     parser.add_argument('--out-stride', type=int, default=16,
                         help='network output stride (default: 8)')
     parser.add_argument('--dataset', type=str, default='pascal',
-                        choices=['pascal', 'coco', 'cityscapes'],
+                        choices=['pascal', 'coco', 'cityscapes', 'kitti', 'kitti_advanced', 'kitti_advanced_manta', 'handmade_dataset', 'handmade_dataset_stereo', 'multimodal_dataset'],
                         help='dataset name (default: pascal)')
-    parser.add_argument('--use-sbd', action='store_true', default=True,
+    parser.add_argument('--use-sbd', action='store_true', default=False,
                         help='whether to use SBD dataset (default: True)')
     parser.add_argument('--workers', type=int, default=4,
                         metavar='N', help='dataloader threads')
-    parser.add_argument('--base-size', type=int, default=513,
+    parser.add_argument('--base-size', type=int, default=512,
                         help='base image size')
-    parser.add_argument('--crop-size', type=int, default=513,
+    parser.add_argument('--crop-size', type=int, default=512,
                         help='crop image size')
-    parser.add_argument('--sync-bn', type=bool, default=None,
+    parser.add_argument('--sync-bn', type=bool, default=True,
                         help='whether to use sync bn (default: auto)')
     parser.add_argument('--freeze-bn', type=bool, default=False,
                         help='whether to freeze bn parameters (default: False)')
     parser.add_argument('--loss-type', type=str, default='ce',
-                        choices=['ce', 'focal'],
+                        choices=['ce', 'focal', 'original','bce'],
                         help='loss func type (default: ce)')
     # training hyper params
     parser.add_argument('--epochs', type=int, default=None, metavar='N',
@@ -213,6 +318,8 @@
                                 testing (default: auto)')
     parser.add_argument('--use-balanced-weights', action='store_true', default=False,
                         help='whether to use balanced weights (default: False)')
+    parser.add_argument('--ratio', type=float, default=None, metavar='N',
+                        help='number of ratio in RGFSConv (default: 1)')
     # optimizer params
     parser.add_argument('--lr', type=float, default=None, metavar='LR',
                         help='learning rate (default: auto)')
@@ -247,6 +354,23 @@
     parser.add_argument('--no-val', action='store_true', default=False,
                         help='skip validation during training')
 
+    # propagation and positional encoding option
+    parser.add_argument('--propagation', type=int, default=0,
+                        help='image propagation length (default: 0)')
+    parser.add_argument('--positional-encoding', action='store_true', default=False,
+                        help='use positional encoding')
+    parser.add_argument('--use-aolp', action='store_true', default=False,
+                        help='use aolp')
+    parser.add_argument('--use-dolp', action='store_true', default=False,
+                        help='use dolp')
+    parser.add_argument('--use-nir', action='store_true', default=False,
+                        help='use nir')
+    parser.add_argument('--use-pretrained-resnet', action='store_true', default=False,
+                        help='use pretrained resnet101')
+    parser.add_argument('--list-folder', type=str, default='list_folder1')
+    parser.add_argument('--is-multimodal', action='store_true', default=False,
+                        help='use multihead architecture')
+
     args = parser.parse_args()
     args.cuda = not args.no_cuda and torch.cuda.is_available()
     if args.cuda:
@@ -267,6 +391,8 @@
             'coco': 30,
             'cityscapes': 200,
             'pascal': 50,
+            'kitti': 50,
+            'kitti_advanced': 50
         }
         args.epochs = epoches[args.dataset.lower()]
 
@@ -281,6 +407,8 @@
             'coco': 0.1,
             'cityscapes': 0.01,
             'pascal': 0.007,
+            'kitti' : 0.01,
+            'kitti_advanced' : 0.01
         }
         args.lr = lrs[args.dataset.lower()] / (4 * len(args.gpu_ids)) * args.batch_size
 
@@ -284,12 +412,24 @@
         }
         args.lr = lrs[args.dataset.lower()] / (4 * len(args.gpu_ids)) * args.batch_size
 
-
     if args.checkname is None:
         args.checkname = 'deeplab-'+str(args.backbone)
     print(args)
+    # input('Check arguments! Press Enter...')
+    # os.environ['PYTHONHASHSEED'] = str(args.seed)
+    # random.seed(args.seed)
+    # np.random.seed(args.seed)
     torch.manual_seed(args.seed)
-    trainer = Trainer(args)
+    # torch.cuda.manual_seed(args.seed)
+    # torch.cuda.manual_seed_all(args.seed)
+    # torch.backends.cudnn.deterministic = True
+    # torch.backends.cudnn.benchmark = False
+
+    # trainer = Trainer(args)
+    if args.is_multimodal:
+        print("USE Multimodal Model")
+        trainer = TrainerMultimodal(args)
+    
     print('Starting Epoch:', trainer.args.start_epoch)
     print('Total Epoches:', trainer.args.epochs)
     for epoch in range(trainer.args.start_epoch, trainer.args.epochs):
@@ -298,6 +438,4 @@
             trainer.validation(epoch)
 
     trainer.writer.close()
-
-if __name__ == "__main__":
-   main()
+    print(args)
diff -rdNuB -x .git pytorch-deeplab-xception/train_coco.sh public-multimodal-material-segmentation/train_coco.sh
--- pytorch-deeplab-xception/train_coco.sh	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/train_coco.sh	1970-01-01 09:00:00.000000000 +0900
@@ -1 +0,0 @@
-CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --backbone resnet --lr 0.01 --workers 4 --epochs 40 --batch-size 16 --gpu-ids 0,1,2,3 --checkname deeplab-resnet --eval-interval 1 --dataset coco
diff -rdNuB -x .git pytorch-deeplab-xception/train_voc.sh public-multimodal-material-segmentation/train_voc.sh
--- pytorch-deeplab-xception/train_voc.sh	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/train_voc.sh	1970-01-01 09:00:00.000000000 +0900
@@ -1 +0,0 @@
-CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --backbone resnet --lr 0.007 --workers 4 --use-sbd True --epochs 50 --batch-size 16 --gpu-ids 0,1,2,3 --checkname deeplab-resnet --eval-interval 1 --dataset pascal
Binary files pytorch-deeplab-xception/utils/__pycache__/attr_dict.cpython-36.pyc and public-multimodal-material-segmentation/utils/__pycache__/attr_dict.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/calculate_weights.cpython-36.pyc and public-multimodal-material-segmentation/utils/__pycache__/calculate_weights.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/calculate_weights.cpython-38.pyc and public-multimodal-material-segmentation/utils/__pycache__/calculate_weights.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/loss.cpython-36.pyc and public-multimodal-material-segmentation/utils/__pycache__/loss.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/loss.cpython-38.pyc and public-multimodal-material-segmentation/utils/__pycache__/loss.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/lr_scheduler.cpython-36.pyc and public-multimodal-material-segmentation/utils/__pycache__/lr_scheduler.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/lr_scheduler.cpython-38.pyc and public-multimodal-material-segmentation/utils/__pycache__/lr_scheduler.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/metrics.cpython-36.pyc and public-multimodal-material-segmentation/utils/__pycache__/metrics.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/metrics.cpython-38.pyc and public-multimodal-material-segmentation/utils/__pycache__/metrics.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/saver.cpython-36.pyc and public-multimodal-material-segmentation/utils/__pycache__/saver.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/saver.cpython-38.pyc and public-multimodal-material-segmentation/utils/__pycache__/saver.cpython-38.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/summaries.cpython-36.pyc and public-multimodal-material-segmentation/utils/__pycache__/summaries.cpython-36.pyc differ
Binary files pytorch-deeplab-xception/utils/__pycache__/summaries.cpython-38.pyc and public-multimodal-material-segmentation/utils/__pycache__/summaries.cpython-38.pyc differ
diff -rdNuB -x .git pytorch-deeplab-xception/utils/attr_dict.py public-multimodal-material-segmentation/utils/attr_dict.py
--- pytorch-deeplab-xception/utils/attr_dict.py	1970-01-01 09:00:00.000000000 +0900
+++ public-multimodal-material-segmentation/utils/attr_dict.py	2022-03-24 20:37:30.252648062 +0900
@@ -0,0 +1,72 @@
+"""
+# Code adapted from:
+# https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/collections.py
+
+Source License
+# Copyright (c) 2017-present, Facebook, Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+##############################################################################
+#
+# Based on:
+# --------------------------------------------------------
+# Fast R-CNN
+# Copyright (c) 2015 Microsoft
+# Licensed under The MIT License [see LICENSE for details]
+# Written by Ross Girshick
+# --------------------------------------------------------
+"""
+
+class AttrDict(dict):
+
+    IMMUTABLE = '__immutable__'
+
+    def __init__(self, *args, **kwargs):
+        super(AttrDict, self).__init__(*args, **kwargs)
+        self.__dict__[AttrDict.IMMUTABLE] = False
+
+    def __getattr__(self, name):
+        if name in self.__dict__:
+            return self.__dict__[name]
+        elif name in self:
+            return self[name]
+        else:
+            raise AttributeError(name)
+
+    def __setattr__(self, name, value):
+        if not self.__dict__[AttrDict.IMMUTABLE]:
+            if name in self.__dict__:
+                self.__dict__[name] = value
+            else:
+                self[name] = value
+        else:
+            raise AttributeError(
+                'Attempted to set "{}" to "{}", but AttrDict is immutable'.
+                format(name, value)
+            )
+
+    def immutable(self, is_immutable):
+        """Set immutability to is_immutable and recursively apply the setting
+        to all nested AttrDicts.
+        """
+        self.__dict__[AttrDict.IMMUTABLE] = is_immutable
+        # Recursively set immutable state
+        for v in self.__dict__.values():
+            if isinstance(v, AttrDict):
+                v.immutable(is_immutable)
+        for v in self.values():
+            if isinstance(v, AttrDict):
+                v.immutable(is_immutable)
+
+    def is_immutable(self):
+        return self.__dict__[AttrDict.IMMUTABLE]
diff -rdNuB -x .git pytorch-deeplab-xception/utils/calculate_weights.py public-multimodal-material-segmentation/utils/calculate_weights.py
--- pytorch-deeplab-xception/utils/calculate_weights.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/utils/calculate_weights.py	2022-03-24 20:37:30.252648062 +0900
@@ -24,6 +24,8 @@
         class_weights.append(class_weight)
     ret = np.array(class_weights)
     classes_weights_path = os.path.join(Path.db_root_dir(dataset), dataset+'_classes_weights.npy')
+    classes_weights_path1 = os.path.join(Path.db_root_dir(dataset), dataset+'_classes_weights1.npy')
     np.save(classes_weights_path, ret)
+    np.save(classes_weights_path1, z)
 
     return ret
\ No newline at end of file
diff -rdNuB -x .git pytorch-deeplab-xception/utils/loss.py public-multimodal-material-segmentation/utils/loss.py
--- pytorch-deeplab-xception/utils/loss.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/utils/loss.py	2022-03-24 20:37:30.252648062 +0900
@@ -1,8 +1,33 @@
 import torch
 import torch.nn as nn
+import numpy as np
+
+KITTI_LOSS_WEIGHT = torch.tensor([
+    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # background
+    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # asphalt
+    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # concrete
+    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # metal
+    [0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # road marking
+    [0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # tar
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # fabric, leather
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # glass
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # plaster
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # plastic
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # rubber, vinyl
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # mud sand soil gravel
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # ceramic
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # cobblestone
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], # brick
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.1, 0.5, 0.0, 0.0, 0.0], # grass
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], # wood
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.2, 1.0, 0.0, 0.0, 0.0], # leaf
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], # water
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], # human body
+    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], # sky
+],dtype=torch.float32)
 
 class SegmentationLosses(object):
-    def __init__(self, weight=None, size_average=True, batch_average=True, ignore_index=255, cuda=False):
+    def __init__(self, weight=None, size_average=True, batch_average=False, ignore_index=255, cuda=False):
         self.ignore_index = ignore_index
         self.weight = weight
         self.size_average = size_average
@@ -10,11 +35,16 @@
         self.cuda = cuda
 
     def build_loss(self, mode='ce'):
-        """Choices: ['ce' or 'focal']"""
+        """Choices: ['ce' or 'focal' or 'original']"""
         if mode == 'ce':
-            return self.CrossEntropyLoss
+            # return self.CrossEntropyLoss
+            return self.CrossEntropyLossAdv
         elif mode == 'focal':
             return self.FocalLoss
+        elif mode == 'original':
+            return self.OriginalLoss
+        elif mode == 'bce':
+            return self.BCELoss
         else:
             raise NotImplementedError
 
@@ -24,9 +54,64 @@
                                         size_average=self.size_average)
         if self.cuda:
             criterion = criterion.cuda()
+            
+        loss = criterion(logit, target.long())
+
+        if self.batch_average:
+            loss /= n
+
+        return loss
 
+    def CrossEntropyLossAdv(self, logit, target, mask=None):
+        n, c, h, w = logit.size()
+        #print(target.long().size())
+        #print(mask.size())
+        if mask is not None:
+            assert target.size() == mask.size()
+            target[mask>0] = self.ignore_index
+            #target[mask>0] = 0
+        #print(self.ignore_index)
+        criterion = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_index,
+                                        size_average=self.size_average)
+        #criterion = nn.BCEWithLogitsLoss(weight=self.weight,
+        #                                 size_average=self.size_average)
+        if self.cuda:
+            criterion = criterion.cuda()
+        #print(logit.size())   
+        #target=target.unsqueeze(1) 
+        #loss = criterion(logit, target.long())
+        #a=nn.Sigmoid()
+        #print(a(logit))
+        #print(target.long().size())
+        
         loss = criterion(logit, target.long())
+        #print(loss)
+        if self.batch_average:
+            loss /= n
 
+        return loss
+    def BCELoss(self, logit, target1, mask=None):
+        n, c, h, w = logit.size()
+        loss=0
+        #print(target.long().size())
+        #print(mask.size())
+        if mask is not None:
+            assert target1.size() == mask.size()
+            target1[mask>0] = self.ignore_index
+            #target[mask>0] = 0
+        criterion = nn.BCEWithLogitsLoss(weight=self.weight,
+                                         size_average=self.size_average)
+        if self.cuda:
+            criterion = criterion.cuda()
+        target2=np.zeros((target1.shape[0],20,target1.shape[1],target1.shape[2]))
+        for k in range(target1.shape[0]):
+            for i in range(20):
+                target2[k,i,:,:]=np.int64(target1.cpu().numpy()[k,:,:]==i)
+        target=torch.from_numpy(target2).cuda()
+        for i in range(20):
+            #print(target[:,i,:,:].shape)
+            loss += criterion(logit[:,i,:,:], target[:,i,:,:])
+        #print(loss)
         if self.batch_average:
             loss /= n
 
@@ -50,12 +135,30 @@
 
         return loss
 
+    def OriginalLoss(self, logit, target):
+        n, c, h, w = logit.size()
+        criterion = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_index,
+                                        size_average=self.size_average)
+        if self.cuda:
+            criterion = criterion.cuda()
+        target_weight = KITTI_LOSS_WEIGHT[target.long()].permute((0,3,1,2)).float().cuda()
+        exp_x  = torch.exp(logit)
+        loss = torch.sum(torch.mean(-torch.sum(target_weight * logit,dim=1) + torch.log(torch.sum(exp_x,dim=1)),dim=(1,2)))
+
+        if self.batch_average:
+            loss /= n
+
+        return loss
+
 if __name__ == "__main__":
     loss = SegmentationLosses(cuda=True)
-    a = torch.rand(1, 3, 7, 7).cuda()
+    a = torch.rand(1, 21, 7, 7).cuda()
     b = torch.rand(1, 7, 7).cuda()
+    print(a)
+    print(b)
     print(loss.CrossEntropyLoss(a, b).item())
     print(loss.FocalLoss(a, b, gamma=0, alpha=None).item())
+    print(loss.OriginalLoss(a,b).item())
     print(loss.FocalLoss(a, b, gamma=2, alpha=0.5).item())
 
 
diff -rdNuB -x .git pytorch-deeplab-xception/utils/metrics.py public-multimodal-material-segmentation/utils/metrics.py
--- pytorch-deeplab-xception/utils/metrics.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/utils/metrics.py	2022-03-24 20:37:30.252648062 +0900
@@ -32,13 +32,21 @@
         return FWIoU
 
     def _generate_matrix(self, gt_image, pre_image):
+        
         mask = (gt_image >= 0) & (gt_image < self.num_class)
+        
         label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]
+        #print(gt_image[mask].shape)
         count = np.bincount(label, minlength=self.num_class**2)
         confusion_matrix = count.reshape(self.num_class, self.num_class)
+        
+        #print(MIoU)
+        #print(confusion_matrix)
         return confusion_matrix
 
     def add_batch(self, gt_image, pre_image):
+        #print(gt_image.shape,pre_image.shape)
+
         assert gt_image.shape == pre_image.shape
         self.confusion_matrix += self._generate_matrix(gt_image, pre_image)
 
diff -rdNuB -x .git pytorch-deeplab-xception/utils/saver.py public-multimodal-material-segmentation/utils/saver.py
--- pytorch-deeplab-xception/utils/saver.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/utils/saver.py	2022-03-24 20:37:30.252648062 +0900
@@ -31,7 +31,9 @@
                     path = os.path.join(self.directory, 'experiment_{}'.format(str(run_id)), 'best_pred.txt')
                     if os.path.exists(path):
                         with open(path, 'r') as f:
-                            miou = float(f.readline())
+                            #print(f.readline())
+                            miou = float(f.readline().strip())
+                            #miou = f.readline()
                             previous_miou.append(miou)
                     else:
                         continue
@@ -52,6 +54,8 @@
         p['lr_scheduler'] = self.args.lr_scheduler
         p['loss_type'] = self.args.loss_type
         p['epoch'] = self.args.epochs
+        p['batch_size'] = self.args.batch_size
+        p['use_pretrained_resnet'] = self.args.use_pretrained_resnet
         p['base_size'] = self.args.base_size
         p['crop_size'] = self.args.crop_size
 
diff -rdNuB -x .git pytorch-deeplab-xception/utils/summaries.py public-multimodal-material-segmentation/utils/summaries.py
--- pytorch-deeplab-xception/utils/summaries.py	2022-03-24 21:08:37.582627040 +0900
+++ public-multimodal-material-segmentation/utils/summaries.py	2022-03-24 20:37:30.252648062 +0900
@@ -6,6 +6,8 @@
 
 class TensorboardSummary(object):
     def __init__(self, directory):
+        print('-------------- save DIR -----------------------')
+        print(directory)
         self.directory = directory
 
     def create_summary(self):
@@ -20,4 +22,24 @@
         writer.add_image('Predicted label', grid_image, global_step)
         grid_image = make_grid(decode_seg_map_sequence(torch.squeeze(target[:3], 1).detach().cpu().numpy(),
                                                        dataset=dataset), 3, normalize=False, range=(0, 255))
-        writer.add_image('Groundtruth label', grid_image, global_step)
\ No newline at end of file
+        writer.add_image('Groundtruth label', grid_image, global_step)
+
+    def visualize_validation_image(self, writer, dataset, image, target, output, global_step):
+        grid_image = make_grid(image[:10].clone().cpu().data, 10, normalize=True)
+        writer.add_image('Val Image', grid_image, global_step)
+        grid_image = make_grid(decode_seg_map_sequence(torch.max(output[:10], 1)[1].detach().cpu().numpy(),
+                                                       dataset=dataset), 10, normalize=False, range=(0, 255))
+        writer.add_image('Val Predicted label', grid_image, global_step)
+        grid_image = make_grid(decode_seg_map_sequence(torch.squeeze(target[:10], 1).detach().cpu().numpy(),
+                                                       dataset=dataset), 10, normalize=False, range=(0, 255))
+        writer.add_image('Val Groundtruth label', grid_image, global_step)
+
+    def visualize_test_image(self, writer, dataset, image, target, output, global_step):
+        grid_image = make_grid(image.clone().cpu().data, image.size(0), normalize=True)
+        writer.add_image('Test Image', grid_image, global_step)
+        grid_image = make_grid(decode_seg_map_sequence(torch.max(output.float(), 1)[1].detach().cpu().numpy(),
+                                                       dataset=dataset), output.size(0), normalize=False, range=(0, 255))
+        writer.add_image('Test Predicted label', grid_image, global_step)
+        grid_image = make_grid(decode_seg_map_sequence(torch.squeeze(target, 1).detach().cpu().numpy(),
+                                                       dataset=dataset), target.size(0), normalize=False, range=(0, 255))
+        writer.add_image('Test Groundtruth label', grid_image, global_step)
\ No newline at end of file
